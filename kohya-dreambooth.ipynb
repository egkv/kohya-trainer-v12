{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/dev/kohya-dreambooth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![visitors](https://visitor-badge.glitch.me/badge?page_id=linaqruf.kohya-dreambooth)\n",
        "\n",
        "# Kohya Dreambooth<br><small><small>A Colab Notebook For Dreambooth Training\n"
      ],
      "metadata": {
        "id": "slgjeYgd6pWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapted to Google Colab based on [Kohya Guide](https://note.com/kohya_ss/n/nee3ed1649fb6)<br>\n",
        "Adapted to Google Colab by [Linaqruf](https://github.com/Linaqruf)<br>\n",
        "You can find latest notebook update [here](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-dreambooth.ipynb)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gPgBR3KM6E-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Notebook Name | Description | Link |\n",
        "| --- | --- | --- |\n",
        "| [Kohya LoRA Dreambooth](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb) | LoRA Training (Dreambooth method) | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb) |\n",
        "| [Kohya LoRA Fine-Tuning](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-finetuner.ipynb) | LoRA Training (Fine-tune method) | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-finetuner.ipynb) |\n",
        "| [Kohya Trainer](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-trainer.ipynb) | Native Training | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-trainer.ipynb) |\n",
        "| [Kohya Dreambooth](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-dreambooth.ipynb) | Dreambooth Training | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-dreambooth.ipynb) |\n",
        "| Kohya Textual Inversion  | Textual Inversion Training | SOON |\n"
      ],
      "metadata": {
        "id": "OQqyYzkDFA0a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTVqCAgSmie4"
      },
      "source": [
        "# I. Install Kohya Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u3q60di584x",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ## 1.1. Clone Kohya Trainer\n",
        "#@markdown Clone Kohya Trainer from GitHub and check for updates. Use textbox below if you want to checkout other branch or old commit. Leave it empty to stay the HEAD on main.\n",
        "\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "root_dir = \"/content\"\n",
        "%store root_dir\n",
        "repo_dir = str(root_dir)+\"/kohya-trainer\"\n",
        "%store repo_dir\n",
        "tools_dir = str(root_dir)+\"/kohya-trainer/tools\"\n",
        "%store tools_dir \n",
        "finetune_dir = str(root_dir)+\"/kohya-trainer/finetune\"\n",
        "%store finetune_dir\n",
        "training_dir = str(root_dir)+\"/dreambooth\"\n",
        "%store training_dir\n",
        "\n",
        "branch = \"\" #@param {type: \"string\"}\n",
        "repo_url = \"https://github.com/Linaqruf/kohya-trainer\"\n",
        "\n",
        "def clone_repo():\n",
        "  if os.path.isdir(repo_dir):\n",
        "    print(\"The repository folder already exists, will do a !git pull instead\\n\")\n",
        "    %cd {repo_dir}\n",
        "    !git pull origin {branch} if branch else !git pull\n",
        "  else:\n",
        "    %cd {root_dir}\n",
        "    !git clone {repo_url} {repo_dir}\n",
        "\n",
        "if not os.path.isdir(repo_dir):\n",
        "  clone_repo()\n",
        "\n",
        "%cd {root_dir}\n",
        "os.makedirs(repo_dir, exist_ok=True)\n",
        "os.makedirs(tools_dir, exist_ok=True)\n",
        "os.makedirs(finetune_dir, exist_ok=True)\n",
        "os.makedirs(training_dir, exist_ok=True)\n",
        "\n",
        "if branch:\n",
        "  %cd {repo_dir}\n",
        "  status = os.system(f\"git checkout {branch}\")\n",
        "  if status != 0:\n",
        "    raise Exception(\"Failed to checkout branch or commit\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNn0g1pnHfk5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ## 1.2. Installing Dependencies\n",
        "#@markdown This will install required Python packages\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "%cd {repo_dir}\n",
        "\n",
        "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "%store accelerate_config\n",
        "install_xformers = True #@param {'type':'boolean'}\n",
        "\n",
        "def install_dependencies():\n",
        "    !pip -q install --upgrade gallery-dl gdown imjoy-elfinder\n",
        "    !apt -q install liblz4-tool aria2\n",
        "    !pip -q install --upgrade -r requirements.txt\n",
        "\n",
        "    if install_xformers:\n",
        "        !pip -q install -U -I --no-deps https://github.com/camenduru/stable-diffusion-webui-colab/releases/download/0.0.15/xformers-0.0.15.dev0+189828c.d20221207-cp38-cp38-linux_x86_64.whl\n",
        "\n",
        "    from accelerate.utils import write_basic_config\n",
        "    if not os.path.exists(accelerate_config):\n",
        "        write_basic_config(save_location=accelerate_config)\n",
        "\n",
        "install_dependencies()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. Sign-in to Cloud Service"
      ],
      "metadata": {
        "id": "qt9EJv5gQXuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 1.3.1. Login to Huggingface hub\n",
        "from huggingface_hub import login\n",
        "%store -r\n",
        "\n",
        "#@markdown Login to Huggingface hub\n",
        "#@markdown 1. You need a Huggingface account.\n",
        "#@markdown 2. To create a huggingface token, go to https://huggingface.co/settings/tokens, then create a new token or copy an available token with the `Write` role.\n",
        "write_token = \"your-write-token-here\" #@param {type:\"string\"}\n",
        "login(write_token, add_to_git_credential=True)\n",
        "\n",
        "%store write_token\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Rl2zERHbBQ9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 1.3.2. Mount Drive (Optional)\n",
        "from google.colab import drive\n",
        "\n",
        "mount_drive = True #@param {type: \"boolean\"}\n",
        "\n",
        "if mount_drive:\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sKL38-WmQsLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.4. Open Special `File Explorer` for Colab (Optional)\n",
        "#@markdown This will work in real-time even while you run other cells.\n",
        "%store -r\n",
        "\n",
        "import threading\n",
        "from google.colab import output\n",
        "from imjoy_elfinder.app import main\n",
        "\n",
        "thread = threading.Thread(target=main, args=[[\"--root-dir=/content\", \"--port=8765\"]])\n",
        "thread.start()\n",
        "\n",
        "open_in_new_tab = True #@param {type:\"boolean\"}\n",
        "\n",
        "if open_in_new_tab:\n",
        "  output.serve_kernel_port_as_window(8765)\n",
        "else:\n",
        "  output.serve_kernel_port_as_iframe(8765, height='500')\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZmIRAxgEQESm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gob9_OwTlwh"
      },
      "source": [
        "# II. Pretrained Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 2.1. Download Available Model \n",
        "import os\n",
        "%store -r\n",
        "\n",
        "%cd {root_dir}\n",
        "\n",
        "installModels = []\n",
        "installv2Models = []\n",
        "\n",
        "#@markdown ### Available Model\n",
        "#@markdown Select one of available model to download:\n",
        "\n",
        "#@markdown ### SD1.x model\n",
        "modelUrl = [\"\", \\\n",
        "            \"https://huggingface.co/Linaqruf/personal-backup/resolve/main/models/animefull-final-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/cag/anything-v3-1/resolve/main/anything-v3-1.safetensors\", \\\n",
        "            \"https://huggingface.co/andite/anything-v4.0/resolve/main/anything-v4.5-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/Rasgeath/self_made_sauce/resolve/main/Kani-anime-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_nsfw.safetensors\", \\\n",
        "            \"https://huggingface.co/gsdf/Counterfeit-V2.0/resolve/main/Counterfeit-V2.0fp16.safetensors\", \\\n",
        "            \"https://huggingface.co/closertodeath/dpepteahands3/resolve/main/dpepteahand3.ckpt\", \\\n",
        "            \"https://huggingface.co/prompthero/openjourney-v2/resolve/main/openjourney-v2.ckpt\", \\\n",
        "            \"https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/dreamlike-diffusion-1.0.ckpt\", \\\n",
        "            \"https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt\"]\n",
        "modelList = [\"\", \\\n",
        "             \"Animefull-final-pruned\", \\\n",
        "             \"Anything-v3-1\", \\\n",
        "             \"Anything-v4-5-pruned\", \\\n",
        "             \"Kani-anime-pruned\", \\\n",
        "             \"AbyssOrangeMix2-nsfw\", \\\n",
        "             \"Counterfeit-v2\", \\\n",
        "             \"DpepTeaHands3\", \\\n",
        "             \"OpenJourney-v2\", \\\n",
        "             \"Dreamlike-diffusion-v1-0\", \\\n",
        "             \"Stable-Diffusion-v1-5\"]\n",
        "modelName = \"Anything-v3-1\"  #@param [\"\", \"Animefull-final-pruned\", \"Anything-v3-1\", \"Anything-v4-5-pruned\", \"Kani-anime-pruned\", \"AbyssOrangeMix2-nsfw\", \"Counterfeit-v2\", \"DpepTeaHands3\", \"OpenJourney-v2\", \"Dreamlike-diffusion-v1-0\", \"Stable-Diffusion-v1-5\"]\n",
        "\n",
        "#@markdown ### SD2.x model\n",
        "v2ModelUrl = [\"\", \\\n",
        "              \"https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.ckpt\", \\\n",
        "              \"https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-ema-pruned.ckpt\", \\\n",
        "              \"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/wd-1-4-anime_e2.ckpt\", \\\n",
        "              \"https://huggingface.co/p1atdev/pd-archive/resolve/main/plat-v1-3-1.safetensors\"]\n",
        "v2ModelList = [\"\", \\\n",
        "              \"stable-diffusion-2-1-base\", \\\n",
        "              \"stable-diffusion-2-1-768v\", \\\n",
        "              \"waifu-diffusion-1-4-anime-e2\", \\\n",
        "              \"plat-diffusion-v1-3-1\"]\n",
        "v2ModelName = \"\" #@param [\"\", \"stable-diffusion-2-1-base\", \"stable-diffusion-2-1-768v\", \"waifu-diffusion-1-4-anime-e2\", \"plat-diffusion-v1-3-1\"]\n",
        "\n",
        "if modelName != \"\":\n",
        "  installModels.append((modelName, modelUrl[modelList.index(modelName)]))\n",
        "if v2ModelName != \"\":\n",
        "  installv2Models.append((v2ModelName, v2ModelUrl[v2ModelList.index(v2ModelName)]))\n",
        "\n",
        "def install(checkpoint_name, url):\n",
        "  ext = \"ckpt\" if url.endswith(\".ckpt\") else \"safetensors\"\n",
        "\n",
        "  hf_token = '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' \n",
        "  user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "  !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {root_dir}/pre_trained_model -o {checkpoint_name}.{ext} \"{url}\"\n",
        "\n",
        "def install_checkpoint():\n",
        "  for model in installModels:\n",
        "    install(model[0], model[1])\n",
        "  for v2model in installv2Models:\n",
        "    install(v2model[0], v2model[1])\n",
        "\n",
        "install_checkpoint()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wmnsZwClN1XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 2.2. Download Custom Model\n",
        "\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "%cd {root_dir}\n",
        "\n",
        "#@markdown ### Custom model\n",
        "modelUrl = \"\" #@param {'type': 'string'}\n",
        "dst = str(root_dir)+\"/pre_trained_model\"\n",
        "\n",
        "if not os.path.exists(dst):\n",
        "    os.makedirs(dst)\n",
        "\n",
        "def install(url):\n",
        "  base_name = os.path.basename(url)\n",
        "\n",
        "  if url.startswith(\"https://drive.google.com\"):\n",
        "    %cd {dst}\n",
        "    !gdown --fuzzy {url}\n",
        "  elif url.startswith(\"https://huggingface.co/\"):\n",
        "    if '/blob/' in url:\n",
        "      url = url.replace('/blob/', '/resolve/')\n",
        "    #@markdown Change this part with your own huggingface token if you need to download your private model\n",
        "    hf_token = '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' #@param {type:\"string\"}\n",
        "    user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {dst} -o {base_name} {url}\n",
        "  else:\n",
        "    !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {dst} -o {url}\n",
        "\n",
        "install(modelUrl)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3LWn6GzNQ4j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SoucgZQ6jgPQ"
      },
      "outputs": [],
      "source": [
        "#@title ## 2.3. Download Available VAE (Optional)\n",
        "%store -r \n",
        "\n",
        "%cd {root_dir}\n",
        "\n",
        "installVae = []\n",
        "#@markdown ### Available VAE\n",
        "#@markdown Select one of the VAEs to download, select `none` for not download VAE:\n",
        "vaeUrl = [\"\", \\\n",
        "          \"https://huggingface.co/Linaqruf/personal-backup/resolve/main/vae/animevae.pt\", \\\n",
        "          \"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime.ckpt\", \\\n",
        "          \"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt\"]\n",
        "vaeList = [\"none\", \\\n",
        "           \"anime.vae.pt\", \\\n",
        "           \"waifudiffusion.vae.pt\", \\\n",
        "           \"stablediffusion.vae.pt\"]\n",
        "vaeName = \"anime.vae.pt\" #@param [\"none\", \"anime.vae.pt\", \"waifudiffusion.vae.pt\", \"stablediffusion.vae.pt\"]\n",
        "\n",
        "installVae.append((vaeName, vaeUrl[vaeList.index(vaeName)]))\n",
        "\n",
        "def install(vae_name, url):\n",
        "  hf_token = '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'\n",
        "  user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "  !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -o vae/{vae_name} \"{url}\"\n",
        "\n",
        "def install_vae():\n",
        "  if vaeName != \"none\":\n",
        "    for vae in installVae:\n",
        "      install(vae[0], vae[1])\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "install_vae()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En9UUwGNMRMM"
      },
      "source": [
        "# III. Data Acquisition\n",
        "\n",
        "You have two options for acquiring your dataset: uploading it to this notebook or bulk downloading images from Danbooru using the image scraper. If you prefer to use your own dataset, simply upload it to Colab's local files.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 3.1. Define Train Data Directory\n",
        "#@markdown Define the location of your training data. This cell will also create a folder based on your input. Leave `reg_data_dir` empty for now.\n",
        "%store -r\n",
        "\n",
        "parent_directory = \"/content/dreambooth/train_data\" #@param {type: \"string\"}\n",
        "%store parent_directory\n",
        "reg_folder_directory = os.path.join(os.path.dirname(parent_directory), \"reg_data\")\n",
        "%store reg_folder_directory\n",
        "\n",
        "reg_repeats = 1 #@param {type: \"integer\"}\n",
        "train_repeats = 10 #@param {type: \"integer\"}\n",
        "concept_name = \"sksneurosama\" #@param {type: \"string\"}\n",
        "class_name = \"1girl\" #@param {type: \"string\"}\n",
        "#@markdown You can run this cell multiple time to add new concepts\n",
        "\n",
        "def get_folder_name(repeats, class_name, concept_name=None):\n",
        "  if class_name:\n",
        "    return f\"{repeats}_{concept_name} {class_name}\" if concept_name else f\"{repeats}_{class_name}\"\n",
        "  return f\"{repeats}_{concept_name}\"\n",
        "\n",
        "train_folder = get_folder_name(train_repeats, class_name, concept_name=concept_name)\n",
        "reg_folder = get_folder_name(reg_repeats, class_name)\n",
        "\n",
        "train_data_dir = os.path.join(parent_directory, train_folder)\n",
        "reg_data_dir = os.path.join(reg_folder_directory, reg_folder)\n",
        "\n",
        "os.makedirs(parent_directory, exist_ok=True)\n",
        "os.makedirs(reg_folder_directory, exist_ok=True)\n",
        "os.makedirs(train_data_dir, exist_ok=True)\n",
        "os.makedirs(reg_data_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kh7CeDqK4l3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 3.2. Download and Extract Zip (.zip)\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "%store -r\n",
        "\n",
        "#@markdown ### Define Zipfile URL or Zipfile Path\n",
        "zipfile_url_or_path = \"https://huggingface.co/datasets/Linaqruf/your-dataset-name/resolve/main/masabodo_dataset.zip\" #@param {'type': 'string'}\n",
        "zipfile_dst = str(root_dir)+\"/zip_file.zip\"\n",
        "extract_to = \"\" #@param {'type': 'string'}\n",
        "\n",
        "if extract_to != \"\":\n",
        "  os.makedirs(extract_to, exist_ok=True)\n",
        "else:\n",
        "  extract_to = train_data_dir\n",
        "\n",
        "#@markdown This will ignore `extract_to` path and automatically extracting to `train_data_dir`\n",
        "is_dataset = True #@param{'type':'boolean'}\n",
        "\n",
        "#@markdown Tick this if you want to extract all files directly to `extract_to` folder, and automatically delete the zip to save the memory\n",
        "auto_unzip_and_delete = True #@param{'type':'boolean'}\n",
        "\n",
        "dirname = os.path.dirname(zipfile_dst)\n",
        "basename = os.path.basename(zipfile_dst)\n",
        "\n",
        "try:\n",
        "  if zipfile_url_or_path.startswith(\"/content\"):\n",
        "    zipfile_dst = zipfile_url_or_path\n",
        "    if auto_unzip_and_delete == False:\n",
        "      if is_dataset:\n",
        "        extract_to = train_data_dir\n",
        "      !unzip -j {zipfile_dst} -d \"{extract_to}\"\n",
        "  elif zipfile_url_or_path.startswith(\"https://drive.google.com\"):\n",
        "    !gdown --fuzzy  {zipfile_url_or_path}\n",
        "  elif zipfile_url_or_path.startswith(\"magnet:?\"):\n",
        "    !aria2c --summary-interval=10 -c -x 10 -k 1M -s 10 {zipfile_url_or_path}\n",
        "  elif zipfile_url_or_path.startswith(\"https://huggingface.co/\"):\n",
        "    if '/blob/' in zipfile_url_or_path:\n",
        "      zipfile_url_or_path = zipfile_url_or_path.replace('/blob/', '/resolve/')\n",
        "\n",
        "    hf_token = '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'\n",
        "    user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {dirname} -o {basename} {zipfile_url_or_path}\n",
        "  else:\n",
        "    !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {dirname} -o {basename} {zipfile_url_or_path}\n",
        "\n",
        "except Exception as e:\n",
        "  print(\"An error occurred while downloading the file:\", e)\n",
        "\n",
        "if is_dataset:\n",
        "  extract_to = train_data_dir\n",
        "\n",
        "if auto_unzip_and_delete:\n",
        "  !unzip -j {zipfile_dst} -d \"{extract_to}\"\n",
        "\n",
        "  files_to_move = (\"meta_cap.json\", \\\n",
        "                   \"meta_cap_dd.json\", \\\n",
        "                   \"meta_lat.json\", \\\n",
        "                   \"meta_clean.json\")\n",
        "\n",
        "  for filename in os.listdir(extract_to):\n",
        "      file_path = os.path.join(extract_to, filename)\n",
        "      if filename in files_to_move:\n",
        "          shutil.move(file_path, os.path.dirname(extract_to))\n",
        "  \n",
        "  path_obj = Path(zipfile_dst)\n",
        "  zipfile_name = path_obj.parts[-1]\n",
        "  \n",
        "  if os.path.isdir(zipfile_dst):\n",
        "    print(\"\\nThis zipfile doesn't exist or has been deleted \\n\")\n",
        "  else:\n",
        "    os.remove(zipfile_dst)\n",
        "    print(f\"\\n{zipfile_name} has been deleted\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eFFHVTWNZGbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "A0t1dfnU5Xkq"
      },
      "outputs": [],
      "source": [
        "#@title ## 3.3. Simple Booru Scraper (Optional)\n",
        "#@markdown Use gallery-dl to scrape images from a booru site using the specified tags\n",
        "import os\n",
        "import html\n",
        "%store -r\n",
        "\n",
        "%cd {root_dir}\n",
        "\n",
        "booru = \"Gelbooru\" #@param [\"\", \"Danbooru\", \"Gelbooru\"]\n",
        "tag1 = \"cleo_(dragalia_lost) \" #@param {type: \"string\"}\n",
        "tag2 = \"\" #@param {type: \"string\"}\n",
        "download_tags = False #@param {type: \"boolean\"}\n",
        "\n",
        "if tag2 != \"\":\n",
        "  tags = tag1 + \"+\" + tag2\n",
        "else:\n",
        "  tags = tag1\n",
        "\n",
        "write_tags = \"--write-tags\" if download_tags == True else \"\"\n",
        "\n",
        "if booru.lower() == \"danbooru\":\n",
        "  !gallery-dl \"https://danbooru.donmai.us/posts?tags={tags}\" {write_tags} -D \"{train_data_dir}\"\n",
        "elif booru.lower() == \"gelbooru\":\n",
        "  !gallery-dl \"https://gelbooru.com/index.php?page=post&s=list&tags={tags}\" {write_tags} -D \"{train_data_dir}\"\n",
        "else:\n",
        "  print(f\"Unknown booru site: {booru}\")\n",
        "\n",
        "if download_tags == True: \n",
        "  files = [f for f in os.listdir(train_data_dir) if f.endswith(\".txt\")]\n",
        "\n",
        "  for file in files:\n",
        "      file_path = os.path.join(train_data_dir, file)\n",
        "\n",
        "      with open(file_path, \"r\") as f:\n",
        "          contents = f.read()\n",
        "\n",
        "      contents = html.unescape(contents)\n",
        "      contents = contents.replace(\"_\", \" \")\n",
        "      contents = \", \".join(contents.split(\"\\n\"))\n",
        "\n",
        "      with open(file_path, \"w\") as f:\n",
        "          f.write(contents)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IV. Data Preprocessing"
      ],
      "metadata": {
        "id": "zUiL2sLg7swG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 4.1. RGBA to RGB converter (Optional)\n",
        "\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import concurrent.futures\n",
        "from PIL import Image\n",
        "\n",
        "#@markdown Transparent images can negatively impact training results, resulting in good characters with bad backgrounds after the model is finished. To address this issue, this code will convert your transparent dataset with alpha channel (RGBA) to RGB and give it a white background. Alternatively, you can set random colors for a more diverse dataset. \n",
        "\n",
        "#@markdown `NOTE: All images endswith .webp will be deleted after the conversion process is completed.`\n",
        "\n",
        "random_color = False #@param {type:\"boolean\"}\n",
        "\n",
        "batch_size = 32 #@param {type:\"number\"}\n",
        "\n",
        "images = [image for image in os.listdir(train_data_dir) if image.endswith('.png') or image.endswith('.webp')]\n",
        "background_colors = [(255, 255, 255), \n",
        "                     (0, 0, 0), \n",
        "                     (255, 0, 0), \n",
        "                     (0, 255, 0), \n",
        "                     (0, 0, 255), \n",
        "                     (255, 255, 0), \n",
        "                     (255, 0, 255), \n",
        "                     (0, 255, 255)]\n",
        "\n",
        "def process_image(image_name):\n",
        "    img = Image.open(f'{train_data_dir}/{image_name}')\n",
        "\n",
        "    if img.mode in ('RGBA', 'LA'):\n",
        "        if random_color:\n",
        "          background_color = random.choice(background_colors)\n",
        "        else:\n",
        "          background_color = (255, 255, 255)\n",
        "        bg = Image.new('RGB', img.size, background_color)\n",
        "        bg.paste(img, mask=img.split()[-1])\n",
        "\n",
        "        if image_name.endswith('.webp'):\n",
        "            bg = bg.convert('RGB')\n",
        "            bg.save(f'{train_data_dir}/{image_name.replace(\".webp\", \".jpg\")}', \"JPEG\")\n",
        "            os.remove(f'{train_data_dir}/{image_name}')\n",
        "            print(f\" Converted image: {image_name} to {image_name.replace('.webp', '.jpg')}\")\n",
        "        else:\n",
        "            bg.save(f'{train_data_dir}/{image_name}', \"PNG\")\n",
        "            print(f\" Converted image: {image_name}\")\n",
        "    else:\n",
        "        if image_name.endswith('.webp'):\n",
        "            img.save(f'{train_data_dir}/{image_name.replace(\".webp\", \".jpg\")}', \"JPEG\")\n",
        "            os.remove(f'{train_data_dir}/{image_name}')\n",
        "            print(f\" Converted image: {image_name} to {image_name.replace('.webp', '.jpg')}\")\n",
        "        else:\n",
        "            img.save(f'{train_data_dir}/{image_name}', \"PNG\")\n",
        "\n",
        "num_batches = len(images) // batch_size + 1\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    for i in tqdm(range(num_batches)):\n",
        "        start = i * batch_size\n",
        "        end = start + batch_size\n",
        "        batch = images[start:end]\n",
        "        executor.map(process_image, batch)\n",
        "\n",
        "print(\"All images have been converted\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FzCGNU__caAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 4.2. Image Upscaler (Optional)\n",
        "from IPython.utils import capture\n",
        "%cd {root_dir}\n",
        "\n",
        "#@markdown Useful if your dataset has image with resolution lower than `512x512`, this will take some time and consume more gpu power.\n",
        "upscaler_model = \"RealESRGAN_x4plus_anime_6B\" #@param ['RealESRGAN_x4plus','RealESRNet_x4plus','RealESRGAN_x4plus_anime_6B','RealESRGAN_x2plus','realesr-animevideov3','realesr-general-x4v3']\n",
        "upcale_by = 2.5 #@param {type:\"slider\", min:1, max:5, step:0.1}\n",
        "face_enhance = False #@param {type:\"boolean\"}\n",
        "\n",
        "if not os.path.exists(f\"{root_dir}/Real-ESRGAN\"):\n",
        "  print(\"Installing dependencies, please wait...\")\n",
        "  with capture.capture_output() as cap:\n",
        "    !git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "    %cd {root_dir}/Real-ESRGAN\n",
        "    !pip -q install basicsr\n",
        "    !pip -q install facexlib\n",
        "    !pip -q install gfpgan\n",
        "    !pip -q install -r requirements.txt\n",
        "    !python setup.py develop\n",
        "    del cap\n",
        "\n",
        "%cd {root_dir}/Real-ESRGAN\n",
        "!python inference_realesrgan.py \\\n",
        "  -n \"{upscaler_model}\" \\\n",
        "  -i \"{train_data_dir}\" \\\n",
        "  -o \"{train_data_dir}\" \\\n",
        "  --suffix \"\" \\\n",
        "  --outscale {upcale_by} \\\n",
        "  {\"--face_enhance\" if face_enhance else \"\"}\n",
        "  "
      ],
      "metadata": {
        "cellView": "form",
        "id": "o8JLsTRkgDxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 4.3. Data Cleaning\n",
        "#@markdown This will delete unnecessary files and unsupported media like `.mp4`, `.webm`, and `.gif`\n",
        "%store -r\n",
        "\n",
        "import os\n",
        "\n",
        "%cd {root_dir}\n",
        "\n",
        "test = os.listdir(train_data_dir)\n",
        "\n",
        "#@markdown I recommend to `keep_metadata` especially if you're doing resume training and you have metadata and bucket latents file from previous training like `.npz`, `.txt`, `.caption`, and `json`.\n",
        "keep_metadata = True #@param {'type':'boolean'}\n",
        "\n",
        "if keep_metadata == True:\n",
        "  supported_types = [\".png\", \".jpg\", \".jpeg\", \".bmp\", \".webp\", \".caption\", \".npz\", \".txt\", \".json\"]\n",
        "else:\n",
        "  supported_types = [\".png\", \".jpg\", \".jpeg\", \".bmp\", \".webp\"]\n",
        "\n",
        "for item in test:\n",
        "    file_ext = os.path.splitext(item)[1]\n",
        "    if file_ext not in supported_types:\n",
        "        print(f\"Deleting file {item} from {train_data_dir}\")\n",
        "        os.remove(os.path.join(train_data_dir, item))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ug648uiOvUZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4. Data Annotation (Optional)\n",
        "You can choose to train a model using caption. We're using [GIT: GenerativeImage2Text](https://huggingface.co/models?search=microsoft/git) for image captioning and [Waifu Diffusion 1.4 Tagger](https://huggingface.co/spaces/SmilingWolf/wd-v1-4-tags) for image tagging like danbooru.\n",
        "- Use GIT Captioning for: `Images in General`\n",
        "- Use Waifu Diffusion 1.4 Tagger V2 for: `Anime and manga-styled Images`"
      ],
      "metadata": {
        "id": "qdISafLeyklg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 4.4.1. GIT Captioning\n",
        "import glob\n",
        "%store -r\n",
        "\n",
        "%cd {finetune_dir}\n",
        "\n",
        "#@markdown [GIT: GenerativeImage2Text](https://huggingface.co/models?search=microsoft/git) is an image-to-text model published by [Microsoft](https://github.com/microsoft/GenerativeImage2Text). GIT is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using \"teacher forcing\" on a lot of (image, text) pairs. \n",
        "#@markdown Example: `astronaut riding a horse in space`. \n",
        "\n",
        "batch_size = 8 #@param {type:'number'}\n",
        "max_data_loader_n_workers = 2 #@param {type:'number'}\n",
        "model = \"microsoft/git-large-textcaps\" #@param [\"microsoft/git-base\", \"microsoft/git-large-coco\", \"microsoft/git-base-coco\", \"microsoft/git-base-vatex\", \"microsoft/git-base-textcaps\", \"microsoft/git-large-vatex\", \"microsoft/git-base-textvqa\", \"microsoft/git-large-textcaps\", \"microsoft/git-large-r\", \"microsoft/git-base-vqav2\", \"microsoft/git-large-r-textcaps\", \"microsoft/git-large\", \"microsoft/git-large-vqav2\", \"microsoft/git-large-textvqa\", \"microsoft/git-base-msrvtt-qa\", \"microsoft/git-large-msrvtt-qa\", \"microsoft/git-large-r-coco\"]\n",
        "max_length = 50 #@param {type:\"slider\", min:0, max:100, step:1.0}\n",
        "undesired_words = \"\" #@param {type:'string'}\n",
        "\n",
        "!python make_captions_by_git.py \\\n",
        "  \"{train_data_dir}\" \\\n",
        "  --model_id {model} \\\n",
        "  --batch_size {batch_size} \\\n",
        "  --max_length {max_length} \\\n",
        "  --remove_words {undesired_words} \\\n",
        "  --caption_extension .caption \\\n",
        "  --max_data_loader_n_workers {max_data_loader_n_workers}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nvPyH-G_Qdha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 4.4.2. Waifu Diffusion 1.4 Tagger V2\n",
        "import glob\n",
        "%store -r\n",
        "\n",
        "%cd {finetune_dir}\n",
        "\n",
        "#@markdown [Waifu Diffusion 1.4 Tagger V2](https://huggingface.co/spaces/SmilingWolf/wd-v1-4-tags) is Danbooru-styled image classification developed by [SmilingWolf](https://github.com/SmilingWolf). It's intended to classify anime and manga-like artwork but can be useful for general use.\n",
        "#@markdown Example: `1girl, solo, looking_at_viewer, short_hair, bangs, simple_background`. \n",
        "batch_size = 8 #@param {type:'number'}\n",
        "max_data_loader_n_workers = 2 #@param {type:'number'}\n",
        "model = \"Swin_V2\" #@param [\"Swin_V2\", \"Convnext_V2\", \"ViT_V2\"]\n",
        "#@markdown It's recommended to set threshold higher (i.e. `0.85`) if you train on object or character, and lower the threshold (i.e `0.35`) to train on general, style or environment.\n",
        "threshold = 0.35 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "\n",
        "if model == \"Swin_V2\":\n",
        "  repo_id = \"SmilingWolf/wd-v1-4-swinv2-tagger-v2\"\n",
        "elif model == \"Convnext_V2\":\n",
        "  repo_id = \"SmilingWolf/wd-v1-4-convnext-tagger-v2\"\n",
        "else:\n",
        "  repo_id = \"SmilingWolf/wd-v1-4-vit-tagger-v2\"\n",
        "\n",
        "!python tag_images_by_wd14_tagger.py \\\n",
        "  \"{train_data_dir}\" \\\n",
        "  --batch_size {batch_size} \\\n",
        "  --repo_id {repo_id} \\\n",
        "  --thresh {threshold} \\\n",
        "  --caption_extension .txt \\\n",
        "  --max_data_loader_n_workers {max_data_loader_n_workers}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-BdXV7rAy2ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VII. Training Model\n",
        "\n"
      ],
      "metadata": {
        "id": "yHNbl3O_NSS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.1. Define Important folder\n",
        "from google.colab import drive\n",
        "%store -r\n",
        "\n",
        "v2 = False #@param {type:\"boolean\"}\n",
        "v_parameterization = False #@param {type:\"boolean\"}\n",
        "project_name = \"neurosama\" #@param {type:\"string\"}\n",
        "pretrained_model_name_or_path = \"/content/pre_trained_model/Anything-v3-1.safetensors\" #@param {type:\"string\"}\n",
        "vae = \"\"  #@param {type:\"string\"}\n",
        "#@markdown You need to register parent folder and not where `train_data_dir` located\n",
        "train_folder_directory = \"/content/dreambooth/train_data\" #@param {'type':'string'}\n",
        "%store train_folder_directory\n",
        "reg_folder_directory = \"/content/dreambooth/reg_data\" #@param {'type':'string'}\n",
        "%store reg_folder_directory\n",
        "output_dir = \"/content/dreambooth/output\" #@param {'type':'string'}\n",
        "resume_path =\"\" #@param {type:\"string\"}\n",
        "inference_url = \"https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/configs/stable-diffusion/\"\n",
        "\n",
        "#@markdown This will ignore `output_dir` defined above, and changed to `/content/drive/MyDrive/fine_tune/output` by default\n",
        "output_to_drive = False #@param {'type':'boolean'}\n",
        "\n",
        "if output_to_drive:\n",
        "  output_dir = \"/content/drive/MyDrive/dreambooth/output\"\n",
        "\n",
        "  if not os.path.exists(\"/content/drive\"):\n",
        "    drive.mount('/content/drive')  \n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "if v2 and not v_parameterization:\n",
        "  inference_url += \"v2-inference.yaml\"\n",
        "if v2 and v_parameterization:\n",
        "  inference_url += \"v2-inference-v.yaml\"\n",
        "\n",
        "try:\n",
        "  if v2:\n",
        "    !wget {inference_url} -O {output_dir}/{project_name}.yaml\n",
        "    print(\"File successfully downloaded\")\n",
        "except:\n",
        "  print(\"There was an error downloading the file. Please check the URL and try again.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "H_Q23fUEJhnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "import textwrap\n",
        "import yaml\n",
        "\n",
        "%store -r\n",
        "\n",
        "#@title ## 5.3. Start Dreambooth\n",
        "#@markdown ### Define Parameter\n",
        "\n",
        "train_batch_size = 1 #@param {type:\"number\"}\n",
        "max_train_steps = 5000 #@param {type:\"number\"}\n",
        "stop_text_encoder_training = 1000 #@param {type:\"number\"}\n",
        "caption_extension = '.txt' #@param {'type':'string'}\n",
        "learning_rate = 2e-6 #@param {'type':'number'}\n",
        "lr_scheduler = \"constant\" #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"] {allow-input: false}\n",
        "mixed_precision = \"fp16\" #@param [\"no\",\"fp16\",\"bf16\"] {allow-input: false}\n",
        "save_precision = \"fp16\" #@param [\"float\", \"fp16\", \"bf16\"] {allow-input: false}\n",
        "save_n_epochs_type = \"save_n_epoch_ratio\" #@param [\"save_every_n_epochs\", \"save_n_epoch_ratio\"] {allow-input: false}\n",
        "save_n_epochs_type_value = 1 #@param {type:\"number\"}\n",
        "save_model_as = \"safetensors\" #@param [\"ckpt\", \"safetensors\", \"diffusers\", \"diffusers_safetensors\"] {allow-input: false}\n",
        "resolution = 512 #@param {type:\"slider\", min:512, max:1024, step:128}\n",
        "enable_bucket = True #@param {type:\"boolean\"}\n",
        "min_bucket_reso = 320 if resolution > 640 else 256\n",
        "max_bucket_reso = 1280 if resolution > 640 else 1024\n",
        "cache_latents = True #@param {type:\"boolean\"}\n",
        "max_token_length = 225 #@param {type:\"number\"}\n",
        "clip_skip = 2 #@param {type:\"number\"}\n",
        "use_8bit_adam = True #@param {type:\"boolean\"}\n",
        "gradient_checkpointing = False #@param {type:\"boolean\"}\n",
        "gradient_accumulation_steps = 1 #@param {type:\"number\"}\n",
        "seed = 0 #@param {type:\"number\"}\n",
        "logging_dir = \"/content/dreambooth/logs\"\n",
        "log_prefix = project_name\n",
        "additional_argument = \"--save_state --shuffle_caption --xformers\" #@param {type:\"string\"}\n",
        "print_hyperparameter = True #@param {type:\"boolean\"}\n",
        "prior_loss_weight =1.0\n",
        "%cd {repo_dir}\n",
        "\n",
        "train_command=f\"\"\"\n",
        "accelerate launch --config_file={accelerate_config} --num_cpu_threads_per_process=8 train_db.py \\\n",
        "  {\"--v2\" if v2 else \"\"} \\\n",
        "  {\"--v_parameterization\" if v2 and v_parameterization else \"\"} \\\n",
        "  --pretrained_model_name_or_path={pretrained_model_name_or_path} \\\n",
        "  {\"--vae=\" + vae if vae else \"\"} \\\n",
        "  {\"--caption_extension=\" + caption_extension if caption_extension else \"\"} \\\n",
        "  --train_data_dir={train_folder_directory} \\\n",
        "  --reg_data_dir={reg_folder_directory} \\\n",
        "  --output_dir={output_dir} \\\n",
        "  --prior_loss_weight={prior_loss_weight} \\\n",
        "  {\"--resume=\" + resume_path if resume_path else \"\"} \\\n",
        "  {\"--output_name=\" + project_name if project_name else \"\"} \\\n",
        "  --mixed_precision={mixed_precision} \\\n",
        "  --save_precision={save_precision} \\\n",
        "  {\"--save_every_n_epochs=\" + format(save_n_epochs_type_value) if save_n_epochs_type==\"save_every_n_epochs\" else \"\"} \\\n",
        "  {\"--save_n_epoch_ratio=\" + format(save_n_epochs_type_value) if save_n_epochs_type==\"save_n_epoch_ratio\" else \"\"} \\\n",
        "  --save_model_as={save_model_as} \\\n",
        "  --resolution={resolution} \\\n",
        "  {\"--enable_bucket\" if enable_bucket else \"\"} \\\n",
        "  {\"--min_bucket_reso=\" + format(min_bucket_reso) if enable_bucket else \"\"} \\\n",
        "  {\"--max_bucket_reso=\" + format(max_bucket_reso) if enable_bucket else \"\"} \\\n",
        "  {\"--cache_latents\" if cache_latents else \"\"} \\\n",
        "  --train_batch_size={train_batch_size} \\\n",
        "  --max_token_length={max_token_length} \\\n",
        "  {\"--use_8bit_adam\" if use_8bit_adam else \"\"} \\\n",
        "  --max_train_steps={max_train_steps} \\\n",
        "  --stop_text_encoder_training={stop_text_encoder_training} \\\n",
        "  --learning_rate={learning_rate} \\\n",
        "  --lr_scheduler={lr_scheduler} \\\n",
        "  {\"--seed=\" + format(seed) if seed > 0 else \"\"} \\\n",
        "  {\"--gradient_checkpointing\" if gradient_checkpointing else \"\"} \\\n",
        "  {\"--gradient_accumulation_steps=\" + format(gradient_accumulation_steps) } \\\n",
        "  {\"--clip_skip=\" + format(clip_skip) if v2 == False else \"\"} \\\n",
        "  --logging_dir={logging_dir} \\\n",
        "  --log_prefix={log_prefix} \\\n",
        "  {additional_argument}\n",
        "  \"\"\"\n",
        "\n",
        "debug_params = [\"v2\", \\\n",
        "                \"v_parameterization\", \\\n",
        "                \"pretrained_model_name_or_path\", \\\n",
        "                \"vae\", \\\n",
        "                \"caption_extension\", \\\n",
        "                \"train_folder_directory\", \\\n",
        "                \"reg_folder_directory\", \\\n",
        "                \"output_dir\", \\\n",
        "                \"prior_loss_weight\", \\\n",
        "                \"resume_path\", \\\n",
        "                \"project_name\", \\\n",
        "                \"mixed_precision\", \\\n",
        "                \"save_precision\", \\\n",
        "                \"save_n_epochs_type\", \\\n",
        "                \"save_n_epochs_type_value\", \\\n",
        "                \"save_model_as\", \\\n",
        "                \"resolution\", \\\n",
        "                \"enable_bucket\", \\\n",
        "                \"min_bucket_reso\", \\\n",
        "                \"max_bucket_reso\", \\\n",
        "                \"cache_latents\", \\\n",
        "                \"train_batch_size\", \\\n",
        "                \"max_token_length\", \\\n",
        "                \"use_8bit_adam\", \\\n",
        "                \"max_train_steps\", \\\n",
        "                \"stop_text_encoder_training\", \\\n",
        "                \"learning_rate\", \\\n",
        "                \"lr_scheduler\", \\\n",
        "                \"seed\", \\\n",
        "                \"gradient_checkpointing\", \\\n",
        "                \"gradient_accumulation_steps\", \\\n",
        "                \"clip_skip\", \\\n",
        "                \"logging_dir\", \\\n",
        "                \"log_prefix\", \\\n",
        "                \"additional_argument\"]\n",
        "\n",
        "if print_hyperparameter:\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"Hyperparameter\", \"Value\"]\n",
        "    for params in debug_params:\n",
        "        if params != \"\":\n",
        "            if globals()[params] == \"\":\n",
        "                value = \"False\"\n",
        "            else:\n",
        "                value = globals()[params]\n",
        "            table.add_row([params, value])\n",
        "    table.align = \"l\"\n",
        "    print(table)\n",
        "\n",
        "    arg_list = train_command.split()\n",
        "    mod_train_command = {'command': arg_list}\n",
        "    \n",
        "    train_folder = os.path.dirname(output_dir)\n",
        "    \n",
        "    # save the YAML string to a file\n",
        "    with open(str(train_folder)+'/dreambooth_cmd.yaml', 'w') as f:\n",
        "        yaml.dump(mod_train_command, f)\n",
        "\n",
        "f = open(\"./train.sh\", \"w\")\n",
        "f.write(train_command)\n",
        "f.close()\n",
        "!chmod +x ./train.sh\n",
        "!./train.sh"
      ],
      "metadata": {
        "id": "X_Rd3Eh07xlA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqfgyL-thgdw"
      },
      "source": [
        "# VI. Testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 6.1. Inference\n",
        "%store -r\n",
        "\n",
        "v2 = False #@param {type:\"boolean\"}\n",
        "v_parameterization = False #@param {type:\"boolean\"}\n",
        "instance_prompt = \"sksneurosama 1girl\" #@param {type: \"string\"}\n",
        "prompt = \"masterpiece, best quality, 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\" #@param {type: \"string\"}\n",
        "negative = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\" #@param {type: \"string\"}\n",
        "model = \"/content/dreambooth/output/neurosama.safetensors\" #@param {type: \"string\"}\n",
        "vae = \"\" #@param {type: \"string\"}\n",
        "outdir = \"/content/tmp\" #@param {type: \"string\"}\n",
        "scale = 7 #@param {type: \"slider\", min: 1, max: 40}\n",
        "sampler = \"ddim\" #@param [\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
        "steps = 28 #@param {type: \"slider\", min: 1, max: 100}\n",
        "precision = \"fp16\" #@param [\"fp16\", \"bf16\"] {allow-input: false}\n",
        "width = 512 #@param {type: \"integer\"}\n",
        "height = 768 #@param {type: \"integer\"}\n",
        "images_per_prompt = 4 #@param {type: \"integer\"}\n",
        "batch_size = 4 #@param {type: \"integer\"}\n",
        "clip_skip = 2 #@param {type: \"slider\", min: 1, max: 40}\n",
        "seed = -1 #@param {type: \"integer\"}\n",
        "\n",
        "final_prompt = f\"{instance_prompt}, {prompt} --n {negative}\" if instance_prompt else f\"{prompt} --n {negative}\"\n",
        "\n",
        "\n",
        "%cd {repo_dir}\n",
        "\n",
        "!python gen_img_diffusers.py \\\n",
        "  {\"--v2\" if v2 else \"\"} \\\n",
        "  {\"--v_parameterization\" if v2 and v_parameterization else \"\"} \\\n",
        "  --ckpt={model} \\\n",
        "  --outdir={outdir} \\\n",
        "  --xformers \\\n",
        "  {\"--vae=\" + vae if vae else \"\"} \\\n",
        "  --{precision} \\\n",
        "  --W={width} \\\n",
        "  --H={height} \\\n",
        "  {\"--seed=\" + format(seed) if seed > 0 else \"\"} \\\n",
        "  --scale={scale} \\\n",
        "  --sampler={sampler} \\\n",
        "  --steps={steps} \\\n",
        "  --max_embeddings_multiples=3 \\\n",
        "  --batch_size={batch_size} \\\n",
        "  --images_per_prompt={images_per_prompt} \\\n",
        "  {\"--clip_skip=\" + format(clip_skip) if v2 == False else \"\"} \\\n",
        "  --prompt=\"{final_prompt}\"\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "j1jJ4z3AXRO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 6.2. Visualize loss graph (Optional)\n",
        "training_logs_path = \"/content/dreambooth/logs\" #@param {type : \"string\"}\n",
        "\n",
        "%cd {repo_dir}\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {training_logs_path}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jgKi9y4w7tmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VII. Extras"
      ],
      "metadata": {
        "id": "N6ckzE2GWudi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 7.1. Convert Diffusers to `.ckpt/.safetensors`\n",
        "import os\n",
        "%store -r\n",
        "%cd {tools_dir}\n",
        "\n",
        "\n",
        "#@markdown ## Define model path\n",
        "weight = \"/content/dreambooth/output/neurosama.safetensors\" #@param {'type': 'string'}\n",
        "weight_dir = os.path.dirname(weight)\n",
        "base_name = os.path.splitext(os.path.basename(weight))[0]\n",
        "\n",
        "convert = \"ckpt_safetensors_to_diffusers\" #@param [\"diffusers_to_ckpt_safetensors\", \"ckpt_safetensors_to_diffusers\"] {'allow-input': false}\n",
        "#@markdown ___\n",
        "#@markdown ## Conversion Config\n",
        "#@markdown ___\n",
        "#@markdown ### Diffusers to `.ckpt/.safetensors`\n",
        "use_safetensors = False #@param {'type': 'boolean'}\n",
        "\n",
        "save_precision = \"--float\" #@param [\"--fp16\",\"--bf16\",\"--float\"] {'allow-input': false}\n",
        "\n",
        "#@markdown ### `.ckpt/.safetensors` to Diffusers\n",
        "#@markdown is your model v1 or v2 based Stable Diffusion Model\n",
        "version = \"--v1\" #@param [\"--v1\",\"--v2\"] {'allow-input': false}\n",
        "diffusers = os.path.join(weight_dir, base_name)\n",
        "\n",
        "#@markdown Additional file for diffusers\n",
        "feature_extractor = True #@param {'type': 'boolean'}\n",
        "safety_checker = True #@param {'type': 'boolean'}\n",
        "\n",
        "if use_safetensors:\n",
        "    checkpoint = str(diffusers)+\".safetensors\"\n",
        "else:\n",
        "    checkpoint = str(diffusers)+\".ckpt\"\n",
        "\n",
        "if version == \"--v1\":\n",
        "  reference_model = \"runwayml/stable-diffusion-v1-5\"\n",
        "elif version == \"--v2\":\n",
        "  reference_model = \"stabilityai/stable-diffusion-2-1\"\n",
        "\n",
        "if convert == \"diffusers_to_ckpt_safetensors\":\n",
        "    if not weight.endswith(\".ckpt\") or weight.endswith(\".safetensors\"):\n",
        "        !python convert_diffusers20_original_sd.py \\\n",
        "            \"{weight}\" \\\n",
        "            \"{checkpoint}\"\" \\\n",
        "            {save_precision}\n",
        "\n",
        "else:    \n",
        "    !python convert_diffusers20_original_sd.py \\\n",
        "        \"{weight}\" \\\n",
        "        \"{diffusers}\" \\\n",
        "        {version} \\\n",
        "        --reference_model {reference_model} \n",
        "\n",
        "    url1 = \"https://huggingface.co/CompVis/stable-diffusion-safety-checker/resolve/main/preprocessor_config.json\"\n",
        "    url2 = \"https://huggingface.co/CompVis/stable-diffusion-safety-checker/resolve/main/config.json\"\n",
        "    url3 = \"https://huggingface.co/CompVis/stable-diffusion-safety-checker/resolve/main/pytorch_model.bin\"\n",
        "\n",
        "    if feature_extractor == True:\n",
        "      if not os.path.exists(str(diffusers)+'/feature_extractor'):\n",
        "        os.makedirs(str(diffusers)+'/feature_extractor')\n",
        "      \n",
        "      !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d '{diffusers}/feature_extractor' -o 'preprocessor_config.json' {url1}\n",
        "\n",
        "    if safety_checker == True:\n",
        "      if not os.path.exists(str(diffusers)+'/safety_checker'):\n",
        "        os.makedirs(str(diffusers)+'/safety_checker')\n",
        "      \n",
        "      !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d '{diffusers}/safety_checker' -o 'config.json' {url2}\n",
        "      !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d '{diffusers}/safety_checker' -o 'pytorch_model.bin' {url3}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EHOvjWCHa-JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "%store -r\n",
        "#@title ## 7.2. Model Pruner\n",
        "\n",
        "%cd {tools_dir}\n",
        "\n",
        "if os.path.exists('prune.py'):\n",
        "  pass\n",
        "else:\n",
        "  # Add a comment to explain what the code is doing\n",
        "  # Download the pruning script if it doesn't already exist\n",
        "  !wget https://raw.githubusercontent.com/lopho/stable-diffusion-prune/main/prune.py\n",
        "\n",
        "#@markdown Convert to Float16\n",
        "fp16 = False #@param {'type':'boolean'}\n",
        "#@markdown Use EMA for weights\n",
        "ema = False #@param {'type':'boolean'}\n",
        "#@markdown Strip CLIP weights\n",
        "no_clip = False #@param {'type':'boolean'}\n",
        "#@markdown Strip VAE weights\n",
        "no_vae = False #@param {'type':'boolean'}\n",
        "#@markdown Strip depth model weights\n",
        "no_depth = False #@param {'type':'boolean'}\n",
        "#@markdown Strip UNet weights\n",
        "no_unet = False #@param {'type':'boolean'}\n",
        "\n",
        "input = \"/content/dreambooth/output/neurosama.safetensors\" #@param {'type' : 'string'}\n",
        "\n",
        "\n",
        "# Notify the user that the model is being loaded\n",
        "print(f\"Loading model from {input}\")\n",
        "\n",
        "input_path = os.path.dirname(input)\n",
        "base_name = os.path.basename(input)\n",
        "output_name = base_name.split('.')[0]\n",
        "# Notify the user of the arguments being used\n",
        "if fp16:\n",
        "    print(\"Converting to float16\")\n",
        "    output_name += '-fp16'\n",
        "if ema:\n",
        "    print(\"Using EMA for weights\")\n",
        "    output_name += '-ema'\n",
        "if no_clip:\n",
        "    print(\"Stripping CLIP weights\")\n",
        "    output_name += '-no-clip'\n",
        "if no_vae:\n",
        "    print(\"Stripping VAE weights\")\n",
        "    output_name += '-no-vae'\n",
        "if no_depth:\n",
        "    print(\"Stripping depth model weights\")\n",
        "    output_name += '-no-depth'\n",
        "if no_unet:\n",
        "    print(\"Stripping UNet weights\")\n",
        "    output_name += '-no-unet'\n",
        "output_name += '-pruned'\n",
        "output_path = os.path.join(input_path, output_name + ('.ckpt' if input.endswith(\".ckpt\") else \".safetensors\"))\n",
        "\n",
        "!python3 prune.py \"{input}\" \\\n",
        "  \"{output_path}\" \\\n",
        "  {'--fp16' if fp16 else ''} \\\n",
        "  {'--ema' if ema else ''} \\\n",
        "  {'--no-clip' if no_clip else ''} \\\n",
        "  {'--no-vae' if no_vae else ''} \\\n",
        "  {'--no-depth' if no_depth else ''} \\\n",
        "  {'--no-unet' if no_unet else ''}\n",
        "\n",
        "# Notify the user of the output file location\n",
        "print(f\"Saving pruned model to {output_path}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "g5Iz_ikf29LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 7.3. Compressing model or dataset\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "zip_module = \"zipfile\" #@param [\"zipfile\", \"shutil\", \"pyminizip\", \"zip\"]\n",
        "directory_to_zip = '/content/dreambooth/train_data' #@param {type: \"string\"}\n",
        "output_filename = '/content/train_data.zip' #@param {type: \"string\"}\n",
        "password = \"\" #@param {type: \"string\"}\n",
        "\n",
        "if zip_module == \"zipfile\":\n",
        "    with zipfile.ZipFile(output_filename, 'w') as zip:\n",
        "        for directory_to_zip, dirs, files in os.walk(directory_to_zip):\n",
        "            for file in files:\n",
        "                zip.write(os.path.join(directory_to_zip, file))\n",
        "elif zip_module == \"shutil\":\n",
        "    shutil.make_archive(output_filename, 'zip', directory_to_zip)\n",
        "elif zip_module == \"pyminizip\":\n",
        "    !pip install pyminizip\n",
        "    import pyminizip\n",
        "    for root, dirs, files in os.walk(directory_to_zip):\n",
        "        for file in files:\n",
        "            pyminizip.compress(os.path.join(root, file), \"\", os.path.join(\"*\",output_filename), password, 5)\n",
        "elif zip_module == \"zip\":\n",
        "    !zip -rv -q -j {output_filename} {directory_to_zip}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rLdEpPKTbI1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIII. Deployment"
      ],
      "metadata": {
        "id": "nyIl9BhNXKUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 8.1. Define your Huggingface Repo\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
        "%store -r\n",
        "\n",
        "api = HfApi()\n",
        "user = api.whoami(write_token)\n",
        "\n",
        "#@markdown #### If your model/dataset repo didn't exist, it will automatically create your repo.\n",
        "model_name = \"your-model-name\" #@param{type:\"string\"}\n",
        "dataset_name = \"your-dataset-name\" #@param{type:\"string\"}\n",
        "make_this_model_private = True #@param{type:\"boolean\"}\n",
        "\n",
        "model_repo = user['name']+\"/\"+model_name.strip()\n",
        "datasets_repo = user['name']+\"/\"+dataset_name.strip()\n",
        "\n",
        "if model_name != \"\":\n",
        "  try:\n",
        "      validate_repo_id(model_repo)\n",
        "      api.create_repo(repo_id=model_repo, \n",
        "                      private=make_this_model_private)\n",
        "      print(\"Model Repo didn't exists, creating repo\")\n",
        "      print(\"Model Repo: \",model_repo,\"created!\\n\")\n",
        "\n",
        "  except HfHubHTTPError as e:\n",
        "      print(f\"Model Repo: {model_repo} exists, skipping create repo\\n\")\n",
        "\n",
        "if dataset_name != \"\":\n",
        "  try:\n",
        "      validate_repo_id(datasets_repo)\n",
        "      api.create_repo(repo_id=datasets_repo,\n",
        "                      repo_type=\"dataset\",\n",
        "                      private=make_this_model_private)\n",
        "      print(\"Dataset Repo didn't exists, creating repo\")\n",
        "      print(\"Dataset Repo\",datasets_repo,\"created!\\n\")\n",
        "\n",
        "  except HfHubHTTPError as e:\n",
        "      print(f\"Dataset repo: {datasets_repo} exists, skipping create repo\\n\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QTXsM170GUpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.2. Upload with `hf_hub`"
      ],
      "metadata": {
        "id": "yUNkWbMHcbiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 8.2.1. Upload Model\n",
        "from huggingface_hub import HfApi\n",
        "from pathlib import Path\n",
        "%store -r\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "#@markdown #### This will be uploaded to model repo\n",
        "\n",
        "model_path = \"/content/dreambooth/output/neurosama.safetensors\" #@param {type :\"string\"}\n",
        "path_in_repo = \"neurosama.safetensors\" #@param {type :\"string\"}\n",
        "\n",
        "#@markdown #### Other Information\n",
        "commit_message = \"\" #@param {type :\"string\"}\n",
        "\n",
        "if not commit_message:\n",
        "  commit_message = \"feat: upload \"+project_name+\" dreambooth model\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "  vae_exists = os.path.exists(os.path.join(model_path, 'vae'))\n",
        "  unet_exists = os.path.exists(os.path.join(model_path, 'unet'))\n",
        "  text_encoder_exists = os.path.exists(os.path.join(model_path, 'text_encoder'))\n",
        "    \n",
        "def upload_model(model_paths, is_folder :bool):\n",
        "  path_obj = Path(model_paths)\n",
        "  trained_model = path_obj.parts[-1]\n",
        "  \n",
        "  if path_in_repo:\n",
        "    trained_model = path_in_repo\n",
        "    \n",
        "  if is_folder == True:\n",
        "    print(f\"Uploading {trained_model} to https://huggingface.co/\"+model_repo)\n",
        "    print(f\"Please wait...\")\n",
        "    \n",
        "    if vae_exists and unet_exists and text_encoder_exists:\n",
        "      api.upload_folder(\n",
        "          folder_path=model_paths,\n",
        "          repo_id=model_repo,\n",
        "          commit_message=commit_message,\n",
        "          ignore_patterns=\".ipynb_checkpoints\"\n",
        "          )\n",
        "    else:\n",
        "      api.upload_folder(\n",
        "          folder_path=model_paths,\n",
        "          path_in_repo=trained_model,\n",
        "          repo_id=model_repo,\n",
        "          commit_message=commit_message,\n",
        "          ignore_patterns=\".ipynb_checkpoints\"\n",
        "          )\n",
        "    print(f\"Upload success, located at https://huggingface.co/\"+model_repo+\"/tree/main\\n\")\n",
        "  else: \n",
        "    print(f\"Uploading {trained_model} to https://huggingface.co/\"+model_repo)\n",
        "    print(f\"Please wait...\")\n",
        "            \n",
        "    api.upload_file(\n",
        "        path_or_fileobj=model_paths,\n",
        "        path_in_repo=trained_model,\n",
        "        repo_id=model_repo,\n",
        "        commit_message=commit_message,\n",
        "        )\n",
        "        \n",
        "    print(f\"Upload success, located at https://huggingface.co/\"+model_repo+\"/blob/main/\"+trained_model+\"\\n\")\n",
        "      \n",
        "def upload():\n",
        "    if model_path.endswith((\".ckpt\", \".safetensors\", \".pt\")):\n",
        "      upload_model(model_path, False)\n",
        "    else:\n",
        "      upload_model(model_path, True)\n",
        "\n",
        "upload()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CIeoJA-eO-8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 8.2.2. Upload Dataset\n",
        "from huggingface_hub import HfApi\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import zipfile\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "#@markdown #### This will be compressed to zip and  uploaded to datasets repo, leave it empty if not necessary\n",
        "train_data_path = \"/content/dreambooth/train_data\" #@param {type :\"string\"}\n",
        "last_state_path = \"/content/dreambooth/output/neurosama-state\" #@param {type :\"string\"}\n",
        "#@markdown ##### `Nerd stuff, only if you want to save training logs`\n",
        "logs_path = \"/content/dreambooth/logs\" #@param {type :\"string\"}\n",
        "#@markdown #### Delete zip after upload\n",
        "delete_zip = True #@param {type :\"boolean\"}\n",
        "\n",
        "if project_name !=\"\":\n",
        "  tmp_dataset = \"/content/dreambooth/\"+project_name+\"_dataset\"\n",
        "  tmp_last_state = \"/content/dreambooth/\"+project_name+\"_last_state\"\n",
        "else:\n",
        "  tmp_dataset = \"/content/dreambooth/tmp_dataset\"\n",
        "  tmp_last_state = \"/content/dreambooth/tmp_last_state\"\n",
        "\n",
        "dataset_zip = tmp_dataset + \".zip\"\n",
        "last_state_zip = tmp_last_state + \".zip\"\n",
        "\n",
        "#@markdown #### Other Information\n",
        "commit_message = \"\" #@param {type :\"string\"}\n",
        "\n",
        "if not commit_message:\n",
        "  commit_message = \"feat: upload \"+project_name+\" dataset, optimizer and logs\"\n",
        "\n",
        "tmp_folder = [\"tmp_dataset\", \\\n",
        "              \"tmp_last_state\"]\n",
        "\n",
        "def makedirs(tmp_folders):\n",
        "  if not os.path.isdir(tmp_folders):\n",
        "    os.makedirs(tmp_folders)\n",
        "\n",
        "for folder in tmp_folder:\n",
        "  makedirs(folder)\n",
        "\n",
        "def upload_dataset(dataset_paths, is_zip : bool):\n",
        "  path_obj = Path(dataset_paths)\n",
        "  dataset_name = path_obj.parts[-1]\n",
        "\n",
        "  if is_zip:\n",
        "    print(f\"Uploading {dataset_name} to https://huggingface.co/datasets/\"+datasets_repo)\n",
        "    print(f\"Please wait...\")\n",
        "\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=dataset_paths,\n",
        "        path_in_repo=dataset_name,\n",
        "        repo_id=datasets_repo,\n",
        "        repo_type=\"dataset\",\n",
        "        commit_message=commit_message,\n",
        "    )\n",
        "    print(f\"Upload success, located at https://huggingface.co/datasets/\"+datasets_repo+\"/blob/main/\"+dataset_name+\"\\n\")\n",
        "  else:\n",
        "    print(f\"Uploading {dataset_name} to https://huggingface.co/datasets/\"+datasets_repo)\n",
        "    print(f\"Please wait...\")\n",
        "\n",
        "    api.upload_folder(\n",
        "        folder_path=dataset_paths,\n",
        "        path_in_repo=dataset_name,\n",
        "        repo_id=datasets_repo,\n",
        "        repo_type=\"dataset\",\n",
        "        commit_message=commit_message,\n",
        "        ignore_patterns=\".ipynb_checkpoints\",\n",
        "    )\n",
        "    print(f\"Upload success, located at https://huggingface.co/datasets/\"+datasets_repo+\"/tree/main/\"+dataset_name+\"\\n\")\n",
        "  \n",
        "def zip_file(tmp_folders):\n",
        "    zipfiles = tmp_folders + \".zip\" \n",
        "    with zipfile.ZipFile(zipfiles, 'w') as zip:\n",
        "      for tmp_folders, dirs, files in os.walk(tmp_folders):\n",
        "          for file in files:\n",
        "              zip.write(os.path.join(tmp_folders, file))\n",
        "\n",
        "def move(src_path, dst_path, is_metadata: bool):\n",
        "  files_to_move = [\"meta_cap.json\", \\\n",
        "                   \"meta_cap_dd.json\", \\\n",
        "                   \"meta_lat.json\", \\\n",
        "                   \"meta_clean.json\", \\\n",
        "                   \"meta_final.json\"]\n",
        "\n",
        "  if os.path.exists(src_path):\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "  if is_metadata:\n",
        "    parent_meta_path = os.path.dirname(src_path)\n",
        "\n",
        "    for filename in os.listdir(parent_meta_path):\n",
        "      file_path = os.path.join(parent_meta_path, filename)\n",
        "      if filename in files_to_move:\n",
        "        shutil.move(file_path, dst_path)\n",
        "\n",
        "def upload():\n",
        "  if train_data_path !=\"\": \n",
        "    move(train_data_path, tmp_dataset, False)\n",
        "    zip_file(tmp_dataset)\n",
        "    upload_dataset(dataset_zip, True)\n",
        "    if delete_zip:\n",
        "      os.remove(dataset_zip)\n",
        "\n",
        "  if last_state_path !=\"\":\n",
        "    move(last_state_path, tmp_last_state, False)\n",
        "    zip_file(tmp_last_state)\n",
        "    upload_dataset(last_state_zip, True)\n",
        "    if delete_zip:\n",
        "      os.remove(last_state_zip)\n",
        "    \n",
        "  if logs_path !=\"\":\n",
        "    upload_dataset(logs_path, False)\n",
        "\n",
        "upload()\n",
        "  "
      ],
      "metadata": {
        "cellView": "form",
        "id": "IW-hS9jnmf-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.3. Commit using Git (Alternative)"
      ],
      "metadata": {
        "id": "CKZpg4keWS5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 8.3.1. Clone Repository\n",
        "\n",
        "clone_model = True #@param {'type': 'boolean'}\n",
        "clone_dataset = True #@param {'type': 'boolean'}\n",
        "\n",
        "!git lfs install --skip-smudge\n",
        "!export GIT_LFS_SKIP_SMUDGE=1\n",
        "\n",
        "if clone_model:\n",
        "  !git clone https://huggingface.co/{model_repo} /content/{model_name}\n",
        "if clone_dataset:\n",
        "  !git clone https://huggingface.co/datasets/{datasets_repo} /content/{dataset_name}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6nBlrOrytO9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 8.3.2. Commit using Git \n",
        "%cd {root_dir}\n",
        "\n",
        "#@markdown Tick which repo you want to commit\n",
        "commit_model = True #@param {'type': 'boolean'}\n",
        "commit_dataset = True #@param {'type': 'boolean'}\n",
        "\n",
        "#@markdown Set **git commit identity**\n",
        "email = \"your-email-here\" #@param {'type': 'string'}\n",
        "name = \"your-username-here\" #@param {'type': 'string'}\n",
        "#@markdown Set **commit message**\n",
        "commit_message = \"\" #@param {type :\"string\"}\n",
        "\n",
        "if not commit_message:\n",
        "  commit_message = \"feat: upload \"+project_name+\" lora model and dataset\"\n",
        "\n",
        "!git config --global user.email \"{email}\"\n",
        "!git config --global user.name \"{name}\"\n",
        "\n",
        "def commit(repo_folder, commit_message):\n",
        "  %cd {root_dir}/{repo_folder}\n",
        "  !git lfs install\n",
        "  !huggingface-cli lfs-enable-largefiles .\n",
        "  !git add .\n",
        "  !git commit -m \"{commit_message}\"\n",
        "  !git push\n",
        "\n",
        "commit(model_name, commit_message)\n",
        "commit(dataset_name, commit_message)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7bJev4PzOFFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Become a supporter!\n",
        "[![ko-fi](https://ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/linaqruf)\n",
        "<a href=\"https://saweria.co/linaqruf\"><img alt=\"Saweria\" src=\"https://img.shields.io/badge/Saweria-7B3F00?style=for-the-badge&logo=ko-fi&logoColor=white\"/></a>\n"
      ],
      "metadata": {
        "id": "uZ3lmPBIFD6a"
      }
    }
  ]
}