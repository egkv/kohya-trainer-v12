{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/dev/kohya-LoRA-finetuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slgjeYgd6pWp"
   },
   "source": [
    "![visitors](https://visitor-badge.glitch.me/badge?page_id=linaqruf.lora-finetuning)\n",
    "# Kohya LoRA Fine-Tuning<br><small><small>A Colab Notebook For LoRA Training (Fine-Tuning Method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPgBR3KM6E-Z"
   },
   "source": [
    "This notebook has been adapted for use in Google Colab based on [kohya-ss/sd-scripts](https://github.com/kohya-ss/sd-scripts). </br>\n",
    "This notebook was adapted by [Linaqruf](https://github.com/Linaqruf)</br>\n",
    "You can find the latest update to the notebook [here](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-finetuner.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zP-eA3rLD5QK"
   },
   "source": [
    "| Notebook Name | Description | Link |\n",
    "| --- | --- | --- |\n",
    "| [Kohya LoRA Dreambooth](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb) | LoRA Training (Dreambooth method) | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb) |\n",
    "| [Kohya LoRA Fine-Tuning](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-finetuner.ipynb) | LoRA Training (Fine-tune method) | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-finetuner.ipynb) |\n",
    "| [Kohya Trainer](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-trainer.ipynb) | Native Training | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-trainer.ipynb) |\n",
    "| [Kohya Dreambooth](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-dreambooth.ipynb) | Dreambooth Training | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-dreambooth.ipynb) |\n",
    "| Kohya Textual Inversion  | Textual Inversion Training | SOON |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTVqCAgSmie4"
   },
   "source": [
    "# I. Install Kohya Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "_u3q60di584x",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr  7 22:52:37 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.86.01    Driver Version: 515.86.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100 80G...  On   | 00000000:43:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    69W / 300W |   2583MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "Stored 'root_dir' (str)\n",
      "Stored 'repo_dir' (str)\n",
      "Stored 'tools_dir' (str)\n",
      "Stored 'finetune_dir' (str)\n",
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "#@title ## 1.1. Clone Kohya Trainer\n",
    "#@markdown Clone Kohya Trainer from GitHub and check for updates. Use textbox below if you want to checkout other branch or old commit. Leave it empty to stay the HEAD on main.\n",
    "\n",
    "import os\n",
    "%store -r\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "root_dir = \"/workspace\"\n",
    "%store root_dir\n",
    "repo_dir = str(root_dir)+\"/kohya-trainer\"\n",
    "%store repo_dir\n",
    "tools_dir = str(root_dir)+\"/kohya-trainer/tools\"\n",
    "%store tools_dir \n",
    "finetune_dir = str(root_dir)+\"/kohya-trainer/finetune\"\n",
    "%store finetune_dir\n",
    "\n",
    "branch = \"\" #@param {type: \"string\"}\n",
    "repo_url = \"https://github.com/egkv/kohya-trainer-v12\"\n",
    "\n",
    "def clone_repo():\n",
    "  if os.path.isdir(repo_dir):\n",
    "    print(\"The repository folder already exists, will do a !git pull instead\\n\")\n",
    "    %cd {repo_dir}\n",
    "    !git pull origin {branch} if branch else !git pull\n",
    "  else:\n",
    "    %cd {root_dir}\n",
    "    !git clone {repo_url} {repo_dir}\n",
    "\n",
    "if not os.path.isdir(repo_dir):\n",
    "  clone_repo()\n",
    "\n",
    "%cd {root_dir}\n",
    "os.makedirs(repo_dir, exist_ok=True)\n",
    "os.makedirs(tools_dir, exist_ok=True)\n",
    "os.makedirs(finetune_dir, exist_ok=True)\n",
    "\n",
    "if branch:\n",
    "  %cd {repo_dir}\n",
    "  status = os.system(f\"git checkout {branch}\")\n",
    "  if status != 0:\n",
    "    raise Exception(\"Failed to checkout branch or commit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "WNn0g1pnHfk5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/kohya-trainer\n",
      "Stored 'accelerate_config' (str)\n",
      "Collecting gallery-dl\n",
      "  Downloading gallery_dl-1.25.1-py3-none-any.whl (599 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.8/599.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gdown in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (4.7.1)\n",
      "Collecting imjoy-elfinder\n",
      "  Downloading imjoy_elfinder-0.1.61-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: requests>=2.11.0 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from gallery-dl) (2.25.1)\n",
      "Requirement already satisfied: filelock in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from gdown) (3.10.7)\n",
      "Requirement already satisfied: tqdm in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from gdown) (4.65.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from gdown) (4.12.0)\n",
      "Requirement already satisfied: six in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from imjoy-elfinder) (4.5.0)\n",
      "Collecting pyramid\n",
      "  Downloading pyramid-2.0.1-py3-none-any.whl (246 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyramid-jinja2\n",
      "  Downloading pyramid_jinja2-2.10-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: werkzeug in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from imjoy-elfinder) (2.2.3)\n",
      "Requirement already satisfied: pillow in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from imjoy-elfinder) (9.4.0)\n",
      "Collecting pathvalidate\n",
      "  Downloading pathvalidate-2.5.2-py3-none-any.whl (20 kB)\n",
      "Collecting waitress\n",
      "  Downloading waitress-2.1.2-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting elfinder-client\n",
      "  Downloading elfinder-client-2.1.55a6.tar.gz (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from requests>=2.11.0->gallery-dl) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from requests>=2.11.0->gallery-dl) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from requests>=2.11.0->gallery-dl) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from requests>=2.11.0->gallery-dl) (2022.12.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.4)\n",
      "Collecting webob>=1.8.3\n",
      "  Downloading WebOb-1.8.7-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.0/115.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting translationstring>=0.4\n",
      "  Downloading translationstring-1.4-py2.py3-none-any.whl (15 kB)\n",
      "Collecting plaster-pastedeploy\n",
      "  Downloading plaster_pastedeploy-1.0.1-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting zope.interface>=3.8.0\n",
      "  Downloading zope.interface-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (246 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting zope.deprecation>=3.5.0\n",
      "  Downloading zope.deprecation-5.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: setuptools in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from pyramid->imjoy-elfinder) (67.6.1)\n",
      "Collecting plaster\n",
      "  Downloading plaster-1.1.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting venusian>=1.0\n",
      "  Downloading venusian-3.0.0-py3-none-any.whl (13 kB)\n",
      "Collecting hupper>=1.5\n",
      "  Downloading hupper-1.12-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: markupsafe in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from pyramid-jinja2->imjoy-elfinder) (2.1.2)\n",
      "Requirement already satisfied: jinja2!=2.11.0,!=2.11.1,!=2.11.2,>=2.5.0 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from pyramid-jinja2->imjoy-elfinder) (3.1.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from requests>=2.11.0->gallery-dl) (1.7.1)\n",
      "Collecting PasteDeploy>=2.0\n",
      "  Downloading PasteDeploy-3.0.1-py3-none-any.whl (16 kB)\n",
      "Building wheels for collected packages: elfinder-client\n",
      "  Building wheel for elfinder-client (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for elfinder-client: filename=elfinder_client-2.1.55a6-py3-none-any.whl size=1262707 sha256=55ae80a4a4aea4a515ddb957f147ba13294e2c96cf17b3c1266c2a85f61f4ce9\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/28/69/0e0707f563860fbeca033c37f60063488b58fdfc283d19a729\n",
      "Successfully built elfinder-client\n",
      "Installing collected packages: translationstring, elfinder-client, zope.interface, zope.deprecation, webob, waitress, venusian, plaster, pathvalidate, PasteDeploy, hupper, plaster-pastedeploy, gallery-dl, pyramid, pyramid-jinja2, imjoy-elfinder\n",
      "Successfully installed PasteDeploy-3.0.1 elfinder-client-2.1.55a6 gallery-dl-1.25.1 hupper-1.12 imjoy-elfinder-0.1.61 pathvalidate-2.5.2 plaster-1.1.2 plaster-pastedeploy-1.0.1 pyramid-2.0.1 pyramid-jinja2-2.10 translationstring-1.4 venusian-3.0.0 waitress-2.1.2 webob-1.8.7 zope.deprecation-5.0 zope.interface-6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Hit:1 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]        \n",
      "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\n",
      "Get:4 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease [18.1 kB] \n",
      "Get:5 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]      \n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]      \n",
      "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [972 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2199 kB]\n",
      "Get:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 Packages [29.5 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1324 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [31.3 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3069 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [28.5 kB]\n",
      "Get:14 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2060 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2590 kB]\n",
      "Get:16 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1028 kB]\n",
      "Fetched 13.7 MB in 2s (6835 kB/s)                           \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libgl1 is already the newest version (1.3.2-1~ubuntu0.20.04.2).\n",
      "libglib2.0-0 is already the newest version (2.64.6-1~ubuntu20.04.4).\n",
      "The following additional packages will be installed:\n",
      "  libaria2-0 libc-ares2 libssh2-1 lz4\n",
      "Suggested packages:\n",
      "  zip\n",
      "The following NEW packages will be installed:\n",
      "  aria2 libaria2-0 libc-ares2 liblz4-tool libssh2-1 lz4 unzip\n",
      "0 upgraded, 7 newly installed, 0 to remove and 16 not upgraded.\n",
      "Need to get 1804 kB of archives.\n",
      "After this operation, 7069 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libc-ares2 amd64 1.15.0-1ubuntu0.2 [36.7 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libssh2-1 amd64 1.8.0-2.1build1 [75.4 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 libaria2-0 amd64 1.35.0-1build1 [1082 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 aria2 amd64 1.35.0-1build1 [356 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 lz4 amd64 1.9.2-2ubuntu0.20.04.1 [82.7 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 unzip amd64 6.0-25ubuntu1.1 [168 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 liblz4-tool all 1.9.2-2ubuntu0.20.04.1 [2524 B]\n",
      "Fetched 1804 kB in 1s (1370 kB/s)     \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package libc-ares2:amd64.\n",
      "(Reading database ... 22521 files and directories currently installed.)\n",
      "Preparing to unpack .../0-libc-ares2_1.15.0-1ubuntu0.2_amd64.deb ...\n",
      "Unpacking libc-ares2:amd64 (1.15.0-1ubuntu0.2) ...\n",
      "Selecting previously unselected package libssh2-1:amd64.\n",
      "Preparing to unpack .../1-libssh2-1_1.8.0-2.1build1_amd64.deb ...\n",
      "Unpacking libssh2-1:amd64 (1.8.0-2.1build1) ...\n",
      "Selecting previously unselected package libaria2-0:amd64.\n",
      "Preparing to unpack .../2-libaria2-0_1.35.0-1build1_amd64.deb ...\n",
      "Unpacking libaria2-0:amd64 (1.35.0-1build1) ...\n",
      "Selecting previously unselected package aria2.\n",
      "Preparing to unpack .../3-aria2_1.35.0-1build1_amd64.deb ...\n",
      "Unpacking aria2 (1.35.0-1build1) ...\n",
      "Selecting previously unselected package lz4.\n",
      "Preparing to unpack .../4-lz4_1.9.2-2ubuntu0.20.04.1_amd64.deb ...\n",
      "Unpacking lz4 (1.9.2-2ubuntu0.20.04.1) ...\n",
      "Selecting previously unselected package unzip.\n",
      "Preparing to unpack .../5-unzip_6.0-25ubuntu1.1_amd64.deb ...\n",
      "Unpacking unzip (6.0-25ubuntu1.1) ...\n",
      "Selecting previously unselected package liblz4-tool.\n",
      "Preparing to unpack .../6-liblz4-tool_1.9.2-2ubuntu0.20.04.1_all.deb ...\n",
      "Unpacking liblz4-tool (1.9.2-2ubuntu0.20.04.1) ...\n",
      "Setting up unzip (6.0-25ubuntu1.1) ...\n",
      "Setting up libc-ares2:amd64 (1.15.0-1ubuntu0.2) ...\n",
      "Setting up lz4 (1.9.2-2ubuntu0.20.04.1) ...\n",
      "Setting up liblz4-tool (1.9.2-2ubuntu0.20.04.1) ...\n",
      "Setting up libssh2-1:amd64 (1.8.0-2.1build1) ...\n",
      "Setting up libaria2-0:amd64 (1.35.0-1build1) ...\n",
      "Setting up aria2 (1.35.0-1build1) ...\n",
      "Processing triggers for mime-support (3.64ubuntu1) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
      "Processing /workspace/kohya-trainer\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting accelerate==0.15.0\n",
      "  Downloading accelerate-0.15.0-py3-none-any.whl (191 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.5/191.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.26.0\n",
      "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (6.1.1)\n",
      "Collecting albumentations\n",
      "  Downloading albumentations-1.3.0-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: opencv-python in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (4.7.0.72)\n",
      "Requirement already satisfied: einops in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.4.1)\n",
      "Collecting einops\n",
      "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diffusers[torch]==0.10.2\n",
      "  Downloading diffusers-0.10.2-py3-none-any.whl (503 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.1/503.1 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytorch_lightning in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.9.4)\n",
      "Collecting pytorch_lightning\n",
      "  Downloading pytorch_lightning-2.0.1-py3-none-any.whl (716 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m716.4/716.4 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bitsandbytes==0.35.0\n",
      "  Downloading bitsandbytes-0.35.0-py3-none-any.whl (62.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard\n",
      "  Downloading tensorboard-2.12.1-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors==0.2.6\n",
      "  Downloading safetensors-0.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (2.25.1)\n",
      "Collecting requests\n",
      "  Using cached requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "Collecting timm==0.4.12\n",
      "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fairscale==0.4.4\n",
      "  Downloading fairscale-0.4.4.tar.gz (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorflow<2.11\n",
      "  Downloading tensorflow-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.1/578.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (0.13.3)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from accelerate==0.15.0->-r requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from accelerate==0.15.0->-r requirements.txt (line 1)) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from accelerate==0.15.0->-r requirements.txt (line 1)) (1.23.3)\n",
      "Requirement already satisfied: psutil in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from accelerate==0.15.0->-r requirements.txt (line 1)) (5.9.4)\n",
      "Requirement already satisfied: torch>=1.4.0 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from accelerate==0.15.0->-r requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from transformers==4.26.0->-r requirements.txt (line 2)) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from transformers==4.26.0->-r requirements.txt (line 2)) (4.65.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from transformers==4.26.0->-r requirements.txt (line 2)) (2023.3.23)\n",
      "Requirement already satisfied: filelock in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from transformers==4.26.0->-r requirements.txt (line 2)) (3.10.7)\n",
      "Collecting importlib-metadata\n",
      "  Downloading importlib_metadata-6.2.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: Pillow in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from diffusers[torch]==0.10.2->-r requirements.txt (line 7)) (9.4.0)\n",
      "Requirement already satisfied: torchvision in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from timm==0.4.12->-r requirements.txt (line 14)) (0.15.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from ftfy->-r requirements.txt (line 3)) (0.2.6)\n",
      "Collecting opencv-python-headless>=4.1.1\n",
      "  Downloading opencv_python_headless-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-image>=0.16.1 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from albumentations->-r requirements.txt (line 4)) (0.19.2)\n",
      "Requirement already satisfied: scipy in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from albumentations->-r requirements.txt (line 4)) (1.10.1)\n",
      "Collecting qudida>=0.0.4\n",
      "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
      "Requirement already satisfied: lightning-utilities>=0.7.0 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from pytorch_lightning->-r requirements.txt (line 8)) (0.8.0)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from pytorch_lightning->-r requirements.txt (line 8)) (2023.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from pytorch_lightning->-r requirements.txt (line 8)) (4.5.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from pytorch_lightning->-r requirements.txt (line 8)) (0.11.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 10)) (1.8.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 10)) (0.40.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 10)) (2.17.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 10)) (67.6.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 10)) (0.7.0)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 10)) (3.20.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 10)) (3.4.3)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 10)) (2.2.3)\n",
      "Requirement already satisfied: absl-py>=0.4 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 10)) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 10)) (1.53.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from requests->-r requirements.txt (line 13)) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from requests->-r requirements.txt (line 13)) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from requests->-r requirements.txt (line 13)) (2.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from requests->-r requirements.txt (line 13)) (3.1.0)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf>=3.19.6\n",
      "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from tensorflow<2.11->-r requirements.txt (line 17)) (1.16.0)\n",
      "Collecting keras<2.11,>=2.10.0\n",
      "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard\n",
      "  Downloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow<2.11\n",
      "  Downloading tensorflow-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.0/578.0 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading tensorflow-2.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow<2.11\n",
      "  Downloading tensorflow-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading tensorflow-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading tensorflow-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading tensorflow-2.8.4-cp310-cp310-manylinux2010_x86_64.whl (498.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.9,>=2.8\n",
      "  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.9,>=2.8.0rc0\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting tensorflow<2.11\n",
      "  Downloading tensorflow-2.8.3-cp310-cp310-manylinux2010_x86_64.whl (498.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.5/498.5 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading tensorflow-2.8.2-cp310-cp310-manylinux2010_x86_64.whl (498.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.0/498.0 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading tensorflow-2.8.1-cp310-cp310-manylinux2010_x86_64.whl (498.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.0/498.0 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of tf-estimator-nightly to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of tensorboard-data-server to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.28.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading requests-2.27.0-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from requests->-r requirements.txt (line 13)) (4.0.0)\n",
      "INFO: pip is looking at multiple versions of tf-estimator-nightly to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading requests-2.25.0-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting chardet<4,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<2.9,>=2.5\n",
      "  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting requests\n",
      "  Downloading requests-2.21.0-py2.py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<1.25,>=1.21.1\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.8/118.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of tensorboard to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (3.8.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 10)) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 10)) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 10)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 10)) (1.3.1)\n",
      "Collecting scikit-learn>=0.19.1\n",
      "  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx>=2.2 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 4)) (3.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 4)) (2023.3.21)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 4)) (2.27.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.15.0->-r requirements.txt (line 1)) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.15.0->-r requirements.txt (line 1)) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.15.0->-r requirements.txt (line 1)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.15.0->-r requirements.txt (line 1)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.15.0->-r requirements.txt (line 1)) (11.7.101)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.15.0->-r requirements.txt (line 1)) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.15.0->-r requirements.txt (line 1)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.15.0->-r requirements.txt (line 1)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.15.0->-r requirements.txt (line 1)) (10.9.0.58)\n",
      "Requirement already satisfied: triton==2.0.0 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.15.0->-r requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: sympy in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.15.0->-r requirements.txt (line 1)) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.15.0->-r requirements.txt (line 1)) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.15.0->-r requirements.txt (line 1)) (2.14.3)\n",
      "Requirement already satisfied: jinja2 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.15.0->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: lit in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.4.0->accelerate==0.15.0->-r requirements.txt (line 1)) (16.0.0)\n",
      "Requirement already satisfied: cmake in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.4.0->accelerate==0.15.0->-r requirements.txt (line 1)) (3.26.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 10)) (2.1.2)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 10)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 10)) (3.2.2)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages (from sympy->torch>=1.4.0->accelerate==0.15.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Building wheels for collected packages: fairscale, library\n",
      "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292837 sha256=2c9e84b918f1720215ff93f95fdcc643a3f35995cfe3ca75c65886e0dc12012e\n",
      "  Stored in directory: /root/.cache/pip/wheels/eb/a6/30/67ce96af10c2038bd71b851f60f388fc7922cc663ed954262d\n",
      "  Building wheel for library (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for library: filename=library-0.0.0-py3-none-any.whl size=34220 sha256=03bcd6501c0f3edec216824b26eeb24134790401d50a6e0530131be8cde17a6e\n",
      "  Stored in directory: /root/.cache/pip/wheels/be/1b/73/3dbe3c43b4cf150bb0aa5b116e27b5470cc7dea9e470b7c0cb\n",
      "Successfully built fairscale library\n",
      "Installing collected packages: safetensors, library, libclang, keras, flatbuffers, bitsandbytes, zipp, wrapt, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, requests, protobuf, opt-einsum, opencv-python-headless, keras-preprocessing, joblib, h5py, google-pasta, gast, einops, astunparse, scikit-learn, importlib-metadata, huggingface-hub, transformers, qudida, google-auth-oauthlib, diffusers, tensorboard, albumentations, tensorflow, accelerate, timm, pytorch_lightning, fairscale\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.3.0\n",
      "    Uninstalling safetensors-0.3.0:\n",
      "      Successfully uninstalled safetensors-0.3.0\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.0\n",
      "    Uninstalling tensorboard-data-server-0.7.0:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.25.1\n",
      "    Uninstalling requests-2.25.1:\n",
      "      Successfully uninstalled requests-2.25.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.0\n",
      "    Uninstalling protobuf-3.20.0:\n",
      "      Successfully uninstalled protobuf-3.20.0\n",
      "  Attempting uninstall: einops\n",
      "    Found existing installation: einops 0.4.1\n",
      "    Uninstalling einops-0.4.1:\n",
      "      Successfully uninstalled einops-0.4.1\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.13.3\n",
      "    Uninstalling huggingface-hub-0.13.3:\n",
      "      Successfully uninstalled huggingface-hub-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.25.1\n",
      "    Uninstalling transformers-4.25.1:\n",
      "      Successfully uninstalled transformers-4.25.1\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 1.0.0\n",
      "    Uninstalling google-auth-oauthlib-1.0.0:\n",
      "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.12.0\n",
      "    Uninstalling accelerate-0.12.0:\n",
      "      Successfully uninstalled accelerate-0.12.0\n",
      "  Attempting uninstall: timm\n",
      "    Found existing installation: timm 0.6.7\n",
      "    Uninstalling timm-0.6.7:\n",
      "      Successfully uninstalled timm-0.6.7\n",
      "  Attempting uninstall: pytorch_lightning\n",
      "    Found existing installation: pytorch-lightning 1.9.4\n",
      "    Uninstalling pytorch-lightning-1.9.4:\n",
      "      Successfully uninstalled pytorch-lightning-1.9.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tb-nightly 2.13.0a20230401 requires google-auth-oauthlib<1.1,>=0.5, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
      "tb-nightly 2.13.0a20230401 requires tensorboard-data-server<0.8.0,>=0.7.0, but you have tensorboard-data-server 0.6.1 which is incompatible.\n",
      "open-clip-torch 2.7.0 requires protobuf==3.20.0, but you have protobuf 3.19.6 which is incompatible.\n",
      "clean-fid 0.1.29 requires requests==2.25.1, but you have requests 2.28.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.15.0 albumentations-1.3.0 astunparse-1.6.3 bitsandbytes-0.35.0 diffusers-0.10.2 einops-0.6.0 fairscale-0.4.4 flatbuffers-23.3.3 gast-0.4.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 h5py-3.8.0 huggingface-hub-0.13.4 importlib-metadata-6.2.0 joblib-1.2.0 keras-2.10.0 keras-preprocessing-1.1.2 libclang-16.0.0 library-0.0.0 opencv-python-headless-4.7.0.72 opt-einsum-3.3.0 protobuf-3.19.6 pytorch_lightning-2.0.1 qudida-0.0.4 requests-2.28.2 safetensors-0.2.6 scikit-learn-1.2.2 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorflow-2.10.1 tensorflow-estimator-2.10.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.2.0 threadpoolctl-3.1.0 timm-0.4.12 transformers-4.26.0 wrapt-1.15.0 zipp-3.15.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 22:58:34.805936: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-07 22:58:34.943949: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-07 22:58:35.471671: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-07 22:58:35.471764: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-07 22:58:35.471772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "#@title ## 1.2. Installing Dependencies\n",
    "#@markdown This will install required Python packages\n",
    "import os\n",
    "%store -r\n",
    "\n",
    "%cd {repo_dir}\n",
    "\n",
    "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
    "%store accelerate_config\n",
    "# colab_ram_patch = True #@param {'type':'boolean'}\n",
    "# install_xformers = True #@param {'type':'boolean'}\n",
    "\n",
    "def install_dependencies():\n",
    "#     if colab_ram_patch:\n",
    "#       !sed -i \"s@cpu@cuda@\" \\\n",
    "#       {repo_dir}/train_db.py \\\n",
    "#       {repo_dir}/train_network.py \\\n",
    "#       {repo_dir}/fine_tune.py \\\n",
    "#       {repo_dir}/library/model_util.py \\\n",
    "#       {repo_dir}/library/train_util.py \n",
    "\n",
    "#       !sed -i \"s@cuda_count@cpu_count@\" \\\n",
    "#       {repo_dir}/train_db.py \\\n",
    "#       {repo_dir}/train_network.py \\\n",
    "#       {repo_dir}/fine_tune.py \\\n",
    "#       {repo_dir}/library/model_util.py \\\n",
    "#       {repo_dir}/library/train_util.py \n",
    "    !pip install --upgrade gallery-dl gdown imjoy-elfinder\n",
    "    !apt-get update && apt-get install unzip -y libgl1 libglib2.0-0 liblz4-tool aria2\n",
    "    !pip install --upgrade -r requirements.txt\n",
    "\n",
    "    # if install_xformers:\n",
    "    #     !pip -q install -U -I --no-deps https://github.com/camenduru/stable-diffusion-webui-colab/releases/download/0.0.15/xformers-0.0.15.dev0+189828c.d20221207-cp38-cp38-linux_x86_64.whl\n",
    "\n",
    "    from accelerate.utils import write_basic_config\n",
    "    if not os.path.exists(accelerate_config):\n",
    "        write_basic_config(save_location=accelerate_config)\n",
    "\n",
    "install_dependencies()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qt9EJv5gQXuB"
   },
   "source": [
    "## 1.3. Sign-in to Cloud Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Rl2zERHbBQ9W"
   },
   "outputs": [],
   "source": [
    "#@title ### 1.3.1. Login to Huggingface hub\n",
    "from huggingface_hub import login\n",
    "%store -r\n",
    "\n",
    "#@markdown Login to Huggingface hub\n",
    "#@markdown 1. You need a Huggingface account.\n",
    "#@markdown 2. To create a huggingface token, go to https://huggingface.co/settings/tokens, then create a new token or copy an available token with the `Write` role.\n",
    "write_token = \"your-write-token-here\" #@param {type:\"string\"}\n",
    "login(write_token, add_to_git_credential=True)\n",
    "\n",
    "%store write_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "sKL38-WmQsLN"
   },
   "outputs": [],
   "source": [
    "#@title ### 1.3.2. Mount Drive (Optional)\n",
    "from google.colab import drive\n",
    "\n",
    "mount_drive = True #@param {type: \"boolean\"}\n",
    "\n",
    "if mount_drive:\n",
    "  drive.mount('/workspace/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ZmIRAxgEQESm"
   },
   "outputs": [],
   "source": [
    "#@title 1.4. Open Special `File Explorer` for Colab (Optional)\n",
    "#@markdown This will work in real-time even while you run other cells.\n",
    "%store -r\n",
    "\n",
    "import threading\n",
    "from google.colab import output\n",
    "from imjoy_elfinder.app import main\n",
    "\n",
    "thread = threading.Thread(target=main, args=[[\"--root-dir=/workspace\", \"--port=8765\"]])\n",
    "thread.start()\n",
    "\n",
    "open_in_new_tab = True #@param {type:\"boolean\"}\n",
    "\n",
    "if open_in_new_tab:\n",
    "  output.serve_kernel_port_as_window(8765)\n",
    "else:\n",
    "  output.serve_kernel_port_as_iframe(8765, height='500')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gob9_OwTlwh"
   },
   "source": [
    "# II. Pretrained Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "wmnsZwClN1XL"
   },
   "outputs": [],
   "source": [
    "#@title ## 2.1. Download Available Model \n",
    "import os\n",
    "%store -r\n",
    "\n",
    "%cd {root_dir}\n",
    "\n",
    "installModels = []\n",
    "installv2Models = []\n",
    "\n",
    "#@markdown ### Available Model\n",
    "#@markdown Select one of available model to download:\n",
    "\n",
    "#@markdown ### SD1.x model\n",
    "modelUrl = [\"\", \\\n",
    "            \"https://huggingface.co/Linaqruf/personal-backup/resolve/main/models/animefull-final-pruned.ckpt\", \\\n",
    "            \"https://huggingface.co/cag/anything-v3-1/resolve/main/anything-v3-1.safetensors\", \\\n",
    "            \"https://huggingface.co/andite/anything-v4.0/resolve/main/anything-v4.5-pruned.ckpt\", \\\n",
    "            \"https://huggingface.co/Rasgeath/self_made_sauce/resolve/main/Kani-anime-pruned.ckpt\", \\\n",
    "            \"https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_nsfw.safetensors\", \\\n",
    "            \"https://huggingface.co/gsdf/Counterfeit-V2.0/resolve/main/Counterfeit-V2.0fp16.safetensors\", \\\n",
    "            \"https://huggingface.co/closertodeath/dpepteahands3/resolve/main/dpepteahand3.ckpt\", \\\n",
    "            \"https://huggingface.co/prompthero/openjourney-v2/resolve/main/openjourney-v2.ckpt\", \\\n",
    "            \"https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/dreamlike-diffusion-1.0.ckpt\", \\\n",
    "            \"https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt\"]\n",
    "modelList = [\"\", \\\n",
    "             \"Animefull-final-pruned\", \\\n",
    "             \"Anything-v3-1\", \\\n",
    "             \"Anything-v4-5-pruned\", \\\n",
    "             \"Kani-anime-pruned\", \\\n",
    "             \"AbyssOrangeMix2-nsfw\", \\\n",
    "             \"Counterfeit-v2\", \\\n",
    "             \"DpepTeaHands3\", \\\n",
    "             \"OpenJourney-v2\", \\\n",
    "             \"Dreamlike-diffusion-v1-0\", \\\n",
    "             \"Stable-Diffusion-v1-5\"]\n",
    "modelName = \"Anything-v3-1\"  #@param [\"\", \"Animefull-final-pruned\", \"Anything-v3-1\", \"Anything-v4-5-pruned\", \"Kani-anime-pruned\", \"AbyssOrangeMix2-nsfw\", \"Counterfeit-v2\", \"DpepTeaHands3\", \"OpenJourney-v2\", \"Dreamlike-diffusion-v1-0\", \"Stable-Diffusion-v1-5\"]\n",
    "\n",
    "#@markdown ### SD2.x model\n",
    "v2ModelUrl = [\"\", \\\n",
    "              \"https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.ckpt\", \\\n",
    "              \"https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-ema-pruned.ckpt\", \\\n",
    "              \"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/wd-1-4-anime_e2.ckpt\", \\\n",
    "              \"https://huggingface.co/p1atdev/pd-archive/resolve/main/plat-v1-3-1.safetensors\"]\n",
    "v2ModelList = [\"\", \\\n",
    "              \"stable-diffusion-2-1-base\", \\\n",
    "              \"stable-diffusion-2-1-768v\", \\\n",
    "              \"waifu-diffusion-1-4-anime-e2\", \\\n",
    "              \"plat-diffusion-v1-3-1\"]\n",
    "v2ModelName = \"\" #@param [\"\", \"stable-diffusion-2-1-base\", \"stable-diffusion-2-1-768v\", \"waifu-diffusion-1-4-anime-e2\", \"plat-diffusion-v1-3-1\"]\n",
    "\n",
    "if modelName != \"\":\n",
    "  installModels.append((modelName, modelUrl[modelList.index(modelName)]))\n",
    "if v2ModelName != \"\":\n",
    "  installv2Models.append((v2ModelName, v2ModelUrl[v2ModelList.index(v2ModelName)]))\n",
    "\n",
    "def install(checkpoint_name, url):\n",
    "  ext = \"ckpt\" if url.endswith(\".ckpt\") else \"safetensors\"\n",
    "\n",
    "  hf_token = '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' \n",
    "  user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
    "  !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {root_dir}/pre_trained_model -o {checkpoint_name}.{ext} \"{url}\"\n",
    "\n",
    "def install_checkpoint():\n",
    "  for model in installModels:\n",
    "    install(model[0], model[1])\n",
    "  for v2model in installv2Models:\n",
    "    install(v2model[0], v2model[1])\n",
    "\n",
    "install_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "3LWn6GzNQ4j5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n",
      " *** Download Progress Summary as of Fri Apr  7 23:01:14 2023 ***              m9s\u001b[0m\u001b[35m]\u001b[0m\u001b[0mm\n",
      "===============================================================================\n",
      "[#fab0c7 1.0GiB/1.9GiB(52%) CN:16 DL:107MiB ETA:8s]\n",
      "FILE: /workspace/pre_trained_model/anyloraCheckpoint_novaeFp16Pruned.ckpt\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[35m[\u001b[0m#fab0c7 1.9GiB/1.9GiB\u001b[36m(99%)\u001b[0m CN:2 DL:\u001b[32m105MiB\u001b[0m\u001b[35m]\u001b[0m\u001b[0mm0m\u001b[35m]\u001b[0m\u001b[0m\n",
      "Download Results:\n",
      "gid   |stat|avg speed  |path/URI\n",
      "======+====+===========+=======================================================\n",
      "fab0c7|\u001b[1;32mOK\u001b[0m  |   105MiB/s|/workspace/pre_trained_model/anyloraCheckpoint_novaeFp16Pruned.ckpt\n",
      "\n",
      "Status Legend:\n",
      "(OK):download completed.\n"
     ]
    }
   ],
   "source": [
    "#@title ## 2.2. Download Custom Model\n",
    "\n",
    "import os\n",
    "%store -r\n",
    "\n",
    "%cd {root_dir}\n",
    "\n",
    "#@markdown ### Custom model\n",
    "modelUrl = \"https://civitai.com/api/download/models/28562\" #@param {'type': 'string'}\n",
    "dst = str(root_dir)+\"/pre_trained_model\"\n",
    "\n",
    "if not os.path.exists(dst):\n",
    "    os.makedirs(dst)\n",
    "\n",
    "def install(url):\n",
    "  base_name = os.path.basename(url)\n",
    "\n",
    "  if url.startswith(\"https://drive.google.com\"):\n",
    "    %cd {dst}\n",
    "    !gdown --fuzzy {url}\n",
    "  elif url.startswith(\"https://huggingface.co/\"):\n",
    "    if '/blob/' in url:\n",
    "      url = url.replace('/blob/', '/resolve/')\n",
    "    #@markdown Change this part with your own huggingface token if you need to download your private model\n",
    "    hf_token = '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' #@param {type:\"string\"}\n",
    "    user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
    "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {dst} -o {base_name} {url}\n",
    "  else:\n",
    "    !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {dst} {url}\n",
    "\n",
    "install(modelUrl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "SoucgZQ6jgPQ"
   },
   "outputs": [],
   "source": [
    "#@title ## 2.3. Download Available VAE (Optional)\n",
    "%store -r \n",
    "\n",
    "%cd {root_dir}\n",
    "\n",
    "installVae = []\n",
    "#@markdown ### Available VAE\n",
    "#@markdown Select one of the VAEs to download, select `none` for not download VAE:\n",
    "vaeUrl = [\"\", \\\n",
    "          \"https://huggingface.co/Linaqruf/personal-backup/resolve/main/vae/animevae.pt\", \\\n",
    "          \"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime.ckpt\", \\\n",
    "          \"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt\"]\n",
    "vaeList = [\"none\", \\\n",
    "           \"anime.vae.pt\", \\\n",
    "           \"waifudiffusion.vae.pt\", \\\n",
    "           \"stablediffusion.vae.pt\"]\n",
    "vaeName = \"anime.vae.pt\" #@param [\"none\", \"anime.vae.pt\", \"waifudiffusion.vae.pt\", \"stablediffusion.vae.pt\"]\n",
    "\n",
    "installVae.append((vaeName, vaeUrl[vaeList.index(vaeName)]))\n",
    "\n",
    "def install(vae_name, url):\n",
    "  hf_token = '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'\n",
    "  user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
    "  !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -o vae/{vae_name} \"{url}\"\n",
    "\n",
    "def install_vae():\n",
    "  if vaeName != \"none\":\n",
    "    for vae in installVae:\n",
    "      install(vae[0], vae[1])\n",
    "  else:\n",
    "    pass\n",
    "\n",
    "install_vae()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "En9UUwGNMRMM"
   },
   "source": [
    "# III. Data Acquisition\n",
    "\n",
    "You have two options for acquiring your dataset: uploading it to this notebook or bulk downloading images from Danbooru using the image scraper. If you prefer to use your own dataset, simply upload it to Colab's local files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "T95ErCuCSCDC",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_data_dir' (str)\n",
      "Your train data directory : /workspace/fine_tune/train_data\n"
     ]
    }
   ],
   "source": [
    "#@title ## 3.1. Define Train Data Directory\n",
    "#@markdown Define where your train data will be located. This cell will also create a folder based on your input. \n",
    "#@markdown This folder will be used as the target folder for scraping, tagging, bucketing, and training in the next cell.\n",
    "\n",
    "import os\n",
    "%store -r\n",
    "\n",
    "train_data_dir = \"/workspace/fine_tune/train_data\" #@param {'type' : 'string'}\n",
    "%store train_data_dir\n",
    "\n",
    "if not os.path.exists(train_data_dir):\n",
    "    os.makedirs(train_data_dir)\n",
    "else:\n",
    "    print(f\"{train_data_dir} already exists\\n\")\n",
    "\n",
    "print(f\"Your train data directory : {train_data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "eFFHVTWNZGbp"
   },
   "outputs": [],
   "source": [
    "#@title ## 3.2. Download and Extract Zip (.zip)\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "%store -r\n",
    "\n",
    "#@markdown ### Define Zipfile URL or Zipfile Path\n",
    "zipfile_url_or_path = \"https://huggingface.co/datasets/Linaqruf/your-dataset-name/resolve/main/masabodo_dataset.zip\" #@param {'type': 'string'}\n",
    "zipfile_dst = str(root_dir)+\"/zip_file.zip\"\n",
    "extract_to = \"\" #@param {'type': 'string'}\n",
    "\n",
    "if extract_to != \"\":\n",
    "  os.makedirs(extract_to, exist_ok=True)\n",
    "else:\n",
    "  extract_to = train_data_dir\n",
    "\n",
    "#@markdown This will ignore `extract_to` path and automatically extracting to `train_data_dir`\n",
    "is_dataset = True #@param{'type':'boolean'}\n",
    "\n",
    "#@markdown Tick this if you want to extract all files directly to `extract_to` folder, and automatically delete the zip to save the memory\n",
    "auto_unzip_and_delete = True #@param{'type':'boolean'}\n",
    "\n",
    "dirname = os.path.dirname(zipfile_dst)\n",
    "basename = os.path.basename(zipfile_dst)\n",
    "\n",
    "try:\n",
    "  if zipfile_url_or_path.startswith(\"/workspace\"):\n",
    "    zipfile_dst = zipfile_url_or_path\n",
    "    if auto_unzip_and_delete == False:\n",
    "      if is_dataset:\n",
    "        extract_to = train_data_dir\n",
    "      !unzip -j {zipfile_dst} -d \"{extract_to}\"\n",
    "  elif zipfile_url_or_path.startswith(\"https://drive.google.com\"):\n",
    "    !gdown --fuzzy  {zipfile_url_or_path}\n",
    "  elif zipfile_url_or_path.startswith(\"magnet:?\"):\n",
    "    !aria2c --summary-interval=10 -c -x 10 -k 1M -s 10 {zipfile_url_or_path}\n",
    "  elif zipfile_url_or_path.startswith(\"https://huggingface.co/\"):\n",
    "    if '/blob/' in zipfile_url_or_path:\n",
    "      zipfile_url_or_path = zipfile_url_or_path.replace('/blob/', '/resolve/')\n",
    "\n",
    "    hf_token = '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'\n",
    "    user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
    "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {dirname} -o {basename} {zipfile_url_or_path}\n",
    "  else:\n",
    "    !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {dirname} -o {basename} {zipfile_url_or_path}\n",
    "\n",
    "except Exception as e:\n",
    "  print(\"An error occurred while downloading the file:\", e)\n",
    "\n",
    "if is_dataset:\n",
    "  extract_to = train_data_dir\n",
    "\n",
    "if auto_unzip_and_delete:\n",
    "  !unzip -j {zipfile_dst} -d \"{extract_to}\"\n",
    "\n",
    "  files_to_move = (\"meta_cap.json\", \\\n",
    "                   \"meta_cap_dd.json\", \\\n",
    "                   \"meta_lat.json\", \\\n",
    "                   \"meta_clean.json\")\n",
    "\n",
    "  for filename in os.listdir(extract_to):\n",
    "      file_path = os.path.join(extract_to, filename)\n",
    "      if filename in files_to_move:\n",
    "          shutil.move(file_path, os.path.dirname(extract_to))\n",
    "  \n",
    "  path_obj = Path(zipfile_dst)\n",
    "  zipfile_name = path_obj.parts[-1]\n",
    "  \n",
    "  if os.path.isdir(zipfile_dst):\n",
    "    print(\"\\nThis zipfile doesn't exist or has been deleted \\n\")\n",
    "  else:\n",
    "    os.remove(zipfile_dst)\n",
    "    print(f\"\\n{zipfile_name} has been deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Kt1GzntK_apb"
   },
   "outputs": [],
   "source": [
    "#@title ## 3.3. Simple Booru Scraper (Optional)\n",
    "#@markdown Use gallery-dl to scrape images from a booru site using the specified tags\n",
    "import os\n",
    "import html\n",
    "%store -r \n",
    "\n",
    "%cd {root_dir}\n",
    "\n",
    "booru = \"Gelbooru\" #@param [\"\", \"Danbooru\", \"Gelbooru\"]\n",
    "tag1 = \"masabodo\" #@param {type: \"string\"}\n",
    "tag2 = \"\" #@param {type: \"string\"}\n",
    "download_tags = True #@param {type: \"boolean\"}\n",
    "\n",
    "if tag2 != \"\":\n",
    "  tags = tag1 + \"+\" + tag2\n",
    "else:\n",
    "  tags = tag1\n",
    "\n",
    "if download_tags == True:\n",
    "  write_tags = \"--write-tags\"\n",
    "else:\n",
    "  write_tags = \"\"\n",
    "\n",
    "if booru.lower() == \"danbooru\":\n",
    "  !gallery-dl \"https://danbooru.donmai.us/posts?tags={tags}\" {write_tags} -D {train_data_dir}\n",
    "elif booru.lower() == \"gelbooru\":\n",
    "  !gallery-dl \"https://gelbooru.com/index.php?page=post&s=list&tags={tags}\" {write_tags} -D {train_data_dir}\n",
    "else:\n",
    "  print(f\"Unknown booru site: {booru}\")\n",
    "\n",
    "if download_tags == True: \n",
    "  files = [f for f in os.listdir(train_data_dir) if f.endswith(\".txt\")]\n",
    "\n",
    "  for file in files:\n",
    "      file_path = os.path.join(train_data_dir, file)\n",
    "      \n",
    "      with open(file_path, \"r\") as f:\n",
    "          contents = f.read()\n",
    "\n",
    "      contents = html.unescape(contents)\n",
    "      contents = contents.replace(\"_\", \" \")\n",
    "      contents = \", \".join(contents.split(\"\\n\"))\n",
    "\n",
    "      with open(file_path, \"w\") as f:\n",
    "          f.write(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-0qKyEgTchp"
   },
   "source": [
    "# IV. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "FzCGNU__caAz",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 141.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Converted image: 20230218_hu_tao_disgust.png\n",
      " Converted image: 20220704_bday_doodlesdeep.png\n",
      " Converted image: 20230224_rida_black.png\n",
      " Converted image: 20220925_rida_bnuy.png\n",
      " Converted image: 20230215_f_the_police.png\n",
      " Converted image: 20220803_rida.png\n",
      " Converted image: 20230215_heart.png\n",
      " Converted image: 20230104_rids.png\n",
      " Converted image: 20230320_SaberLumiEre_flat_hb.png\n",
      " Converted image: 20230405_SaberLumiEre_flat_hb.png\n",
      " Converted image: 20230131_Gooseuncharted_full_hb2.png\n",
      " Converted image: 20230222_pigtails.png\n",
      " Converted image: 20221006_rida_the_suc.png\n",
      " Converted image: 20230127_tie_hair.png\n",
      " Converted image: 20221231_new_year.png\n",
      " Converted image: 20230129_fancy.png\n",
      " Converted image: 20230313_cyber.png\n",
      " Converted image: 20230316_sunny.png\n",
      " Converted image: 20210827_swapped.png\n",
      " Converted image: 20230103_bunida.png\n",
      " Converted image: 20230124_mlem.png\n",
      " Converted image: 20230117_brida.png\n",
      " Converted image: 20230224_narutsuart_flat_b.png\n",
      " Converted image: 20221030_bruh.png\n",
      " Converted image: 20220731_rids.png\n",
      " Converted image: 20221224_xmas.png\n",
      " Converted image: 20230320_morning.png\n",
      " Converted image: 20230208_maida.png\n",
      " Converted image: 20230127_cap.png\n",
      "All images have been converted\n"
     ]
    }
   ],
   "source": [
    "#@title ## 4.1. RGBA to RGB converter (Optional)\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from PIL import Image\n",
    "%store -r\n",
    "#@markdown Transparent images can negatively impact training results, resulting in good characters with bad backgrounds after the model is finished. To address this issue, this code will convert your transparent dataset with alpha channel (RGBA) to RGB and give it a white background. Alternatively, you can set random colors for a more diverse dataset. \n",
    "\n",
    "\n",
    "#@markdown `NOTE: All images endswith .webp will be deleted after the conversion process is completed.`\n",
    "\n",
    "random_color = False #@param {type:\"boolean\"}\n",
    "\n",
    "batch_size = 32 #@param {type:\"number\"}\n",
    "\n",
    "images = [image for image in os.listdir(train_data_dir) if image.endswith('.png') or image.endswith('.webp')]\n",
    "background_colors = [(255, 255, 255), \n",
    "                     (0, 0, 0), \n",
    "                     (255, 0, 0), \n",
    "                     (0, 255, 0), \n",
    "                     (0, 0, 255), \n",
    "                     (255, 255, 0), \n",
    "                     (255, 0, 255), \n",
    "                     (0, 255, 255)]\n",
    "\n",
    "def process_image(image_name):\n",
    "    img = Image.open(f'{train_data_dir}/{image_name}')\n",
    "\n",
    "    if img.mode in ('RGBA', 'LA'):\n",
    "        if random_color:\n",
    "          background_color = random.choice(background_colors)\n",
    "        else:\n",
    "          background_color = (255, 255, 255)\n",
    "        bg = Image.new('RGB', img.size, background_color)\n",
    "        bg.paste(img, mask=img.split()[-1])\n",
    "\n",
    "        if image_name.endswith('.webp'):\n",
    "            bg = bg.convert('RGB')\n",
    "            bg.save(f'{train_data_dir}/{image_name.replace(\".webp\", \".jpg\")}', \"JPEG\")\n",
    "            os.remove(f'{train_data_dir}/{image_name}')\n",
    "            print(f\" Converted image: {image_name} to {image_name.replace('.webp', '.jpg')}\")\n",
    "        else:\n",
    "            bg.save(f'{train_data_dir}/{image_name}', \"PNG\")\n",
    "            print(f\" Converted image: {image_name}\")\n",
    "    else:\n",
    "        if image_name.endswith('.webp'):\n",
    "            img.save(f'{train_data_dir}/{image_name.replace(\".webp\", \".jpg\")}', \"JPEG\")\n",
    "            os.remove(f'{train_data_dir}/{image_name}')\n",
    "            print(f\" Converted image: {image_name} to {image_name.replace('.webp', '.jpg')}\")\n",
    "        else:\n",
    "            img.save(f'{train_data_dir}/{image_name}', \"PNG\")\n",
    "\n",
    "num_batches = len(images) // batch_size + 1\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    for i in tqdm(range(num_batches)):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        batch = images[start:end]\n",
    "        executor.map(process_image, batch)\n",
    "\n",
    "print(\"All images have been converted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "o8JLsTRkgDxj"
   },
   "outputs": [],
   "source": [
    "#@title ## 4.2. Image Upscaler (Optional)\n",
    "from IPython.utils import capture\n",
    "%store -r\n",
    "\n",
    "%cd {root_dir}\n",
    "\n",
    "#@markdown Useful if your dataset has image with resolution lower than `512x512`, this will take some time and consume more gpu power.\n",
    "upscaler_model = \"RealESRGAN_x4plus_anime_6B\" #@param ['RealESRGAN_x4plus','RealESRNet_x4plus','RealESRGAN_x4plus_anime_6B','RealESRGAN_x2plus','realesr-animevideov3','realesr-general-x4v3']\n",
    "upcale_by = 2.5 #@param {type:\"slider\", min:1, max:5, step:0.1}\n",
    "face_enhance = False #@param {type:\"boolean\"}\n",
    "\n",
    "if not os.path.exists(f\"{root_dir}/Real-ESRGAN\"):\n",
    "  print(\"Installing dependencies, please wait...\")\n",
    "  with capture.capture_output() as cap:\n",
    "    !git clone https://github.com/xinntao/Real-ESRGAN.git\n",
    "    %cd {root_dir}/Real-ESRGAN\n",
    "    !pip -q install basicsr\n",
    "    !pip -q install facexlib\n",
    "    !pip -q install gfpgan\n",
    "    !pip -q install -r requirements.txt\n",
    "    !python setup.py develop\n",
    "    del cap\n",
    "\n",
    "%cd {root_dir}/Real-ESRGAN\n",
    "!python inference_realesrgan.py \\\n",
    "  -n \"{upscaler_model}\" \\\n",
    "  -i \"{train_data_dir}\" \\\n",
    "  -o \"{train_data_dir}\" \\\n",
    "  --suffix \"\" \\\n",
    "  --outscale {upcale_by} \\\n",
    "  {\"--face_enhance\" if face_enhance else \"\"}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "Jz2emq6vWnPu",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "#@title ## 4.3. Data Cleaning\n",
    "#@markdown This will delete unnecessary files and unsupported media like `.mp4`, `.webm`, and `.gif`\n",
    "%store -r\n",
    "\n",
    "import os\n",
    "\n",
    "%cd {root_dir}\n",
    "\n",
    "test = os.listdir(train_data_dir)\n",
    "\n",
    "#@markdown I recommend to `keep_metadata` especially if you're doing resume training and you have metadata and bucket latents file from previous training like `.npz`, `.txt`, `.caption`, and `json`.\n",
    "keep_metadata = True #@param {'type':'boolean'}\n",
    "\n",
    "if keep_metadata == True:\n",
    "  supported_types = [\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".caption\", \".npz\", \".txt\", \".json\"]\n",
    "else:\n",
    "  supported_types = [\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\"]\n",
    "\n",
    "for item in test:\n",
    "    file_ext = os.path.splitext(item)[1]\n",
    "    if file_ext not in supported_types:\n",
    "        print(f\"Deleting file {item} from {train_data_dir}\")\n",
    "        os.remove(os.path.join(train_data_dir, item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdISafLeyklg"
   },
   "source": [
    "## 4.4. Data Annotation\n",
    "You can choose to train a model using caption. We're using [GIT: GenerativeImage2Text](https://huggingface.co/models?search=microsoft/git) for image captioning and [Waifu Diffusion 1.4 Tagger](https://huggingface.co/spaces/SmilingWolf/wd-v1-4-tags) for image tagging like danbooru.\n",
    "- Use GIT Captioning for: `Images in General`\n",
    "- Use Waifu Diffusion 1.4 Tagger V2 for: `Anime and manga-styled Images`\n",
    "\n",
    "You can use both in fine-tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nvPyH-G_Qdha"
   },
   "outputs": [],
   "source": [
    "#@title ### 4.4.1. GIT Captioning\n",
    "import glob\n",
    "%store -r\n",
    "\n",
    "%cd {finetune_dir}\n",
    "\n",
    "#@markdown [GIT: GenerativeImage2Text](https://huggingface.co/models?search=microsoft/git) is an image-to-text model published by [Microsoft](https://github.com/microsoft/GenerativeImage2Text). GIT is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using \"teacher forcing\" on a lot of (image, text) pairs. \n",
    "#@markdown Example: `astronaut riding a horse in space`. \n",
    "\n",
    "batch_size = 8 #@param {type:'number'}\n",
    "max_data_loader_n_workers = 2 #@param {type:'number'}\n",
    "model = \"microsoft/git-large-textcaps\" #@param [\"microsoft/git-base\", \"microsoft/git-large-coco\", \"microsoft/git-base-coco\", \"microsoft/git-base-vatex\", \"microsoft/git-base-textcaps\", \"microsoft/git-large-vatex\", \"microsoft/git-base-textvqa\", \"microsoft/git-large-textcaps\", \"microsoft/git-large-r\", \"microsoft/git-base-vqav2\", \"microsoft/git-large-r-textcaps\", \"microsoft/git-large\", \"microsoft/git-large-vqav2\", \"microsoft/git-large-textvqa\", \"microsoft/git-base-msrvtt-qa\", \"microsoft/git-large-msrvtt-qa\", \"microsoft/git-large-r-coco\"]\n",
    "max_length = 50 #@param {type:\"slider\", min:0, max:100, step:1.0}\n",
    "undesired_words = \"\" #@param {type:'string'}\n",
    "\n",
    "!python make_captions_by_git.py \\\n",
    "  \"{train_data_dir}\" \\\n",
    "  --model_id {model} \\\n",
    "  --batch_size {batch_size} \\\n",
    "  --max_length {max_length} \\\n",
    "  --remove_words {undesired_words} \\\n",
    "  --caption_extension .caption \\\n",
    "  --max_data_loader_n_workers {max_data_loader_n_workers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "-BdXV7rAy2ag"
   },
   "outputs": [],
   "source": [
    "#@title ### 4.4.2. Waifu Diffusion 1.4 Tagger V2\n",
    "import glob\n",
    "%store -r\n",
    "\n",
    "%cd {finetune_dir}\n",
    "\n",
    "#@markdown [Waifu Diffusion 1.4 Tagger V2](https://huggingface.co/spaces/SmilingWolf/wd-v1-4-tags) is Danbooru-styled image classification developed by [SmilingWolf](https://github.com/SmilingWolf). It's intended to classify anime and manga-like artwork but can be useful for general use.\n",
    "#@markdown Example: `1girl, solo, looking_at_viewer, short_hair, bangs, simple_background`. \n",
    "batch_size = 8 #@param {type:'number'}\n",
    "max_data_loader_n_workers = 2 #@param {type:'number'}\n",
    "model = \"Swin_V2\" #@param [\"Swin_V2\", \"Convnext_V2\", \"ViT_V2\"]\n",
    "#@markdown It's recommended to set threshold higher (i.e. `0.85`) if you train on object or character, and lower the threshold (i.e `0.35`) to train on general, style or environment.\n",
    "threshold = 0.35 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "\n",
    "if model == \"Swin_V2\":\n",
    "  repo_id = \"SmilingWolf/wd-v1-4-swinv2-tagger-v2\"\n",
    "elif model == \"Convnext_V2\":\n",
    "  repo_id = \"SmilingWolf/wd-v1-4-convnext-tagger-v2\"\n",
    "else:\n",
    "  repo_id = \"SmilingWolf/wd-v1-4-vit-tagger-v2\"\n",
    "\n",
    "!python tag_images_by_wd14_tagger.py \\\n",
    "  \"{train_data_dir}\" \\\n",
    "  --batch_size {batch_size} \\\n",
    "  --repo_id {repo_id} \\\n",
    "  --thresh {threshold} \\\n",
    "  --caption_extension .txt \\\n",
    "  --max_data_loader_n_workers {max_data_loader_n_workers}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "_mLVURhM9PFE"
   },
   "outputs": [],
   "source": [
    "#@title ### 4.4.3. Insert Custom Caption/Tag (Optional)\n",
    "import os\n",
    "%store -r\n",
    "\n",
    "%cd {root_dir}\n",
    "\n",
    "#@markdown If you want to add custom tag, you can do that here. \n",
    "extension = \"txt\" #@param [\"txt\", \"caption\"]\n",
    "custom_tag = \"neurosama\" #@param {type:\"string\"}\n",
    "#@markdown Tick this if you want to append custom tag at the end of lines instead\n",
    "append = True #@param {type:\"boolean\"}\n",
    "#@markdown This will useful for training, you can keep your custom tag separated by comma from being shuffled by `--shuffle_caption`. \n",
    "\n",
    "#@markdown Example: if `keep_tokens = 1`, it will keep the first tag from being shuffled. \n",
    "\n",
    "#@markdown `Note`: by using keep_tokens it will override `append` and automatically add new tag to the first lines instead\n",
    "keep_tokens = 1 #@param {type:\"number\"}\n",
    "\n",
    "def add_tag(filename, tag, append):\n",
    "    with open(filename, \"r\") as f:\n",
    "        contents = f.read()\n",
    "        \t    \t\n",
    "    tag = \", \".join(tag.split())\n",
    "    tag = tag.replace(\"_\", \" \")\n",
    "    \n",
    "    if tag in contents:\n",
    "        return\n",
    "        \n",
    "    if not keep_tokens:\n",
    "      contents = contents.rstrip() + \", \" + tag if append else tag + \", \" + contents\n",
    "    else:\n",
    "      contents = tag + \", \" + contents\n",
    "    \n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(contents)\n",
    "\n",
    "tags = custom_tag.split()\n",
    "for filename in os.listdir(train_data_dir):\n",
    "    if filename.endswith(\".\" + extension):\n",
    "        for tag in tags:\n",
    "            add_tag(os.path.join(train_data_dir, filename), tag, append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "MEHNh0P8Yyxi",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/kohya-trainer/finetune\n",
      "2023-04-07 23:02:57.271856: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-07 23:02:57.412213: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-07 23:02:57.937804: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-07 23:02:57.937879: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-07 23:02:57.937890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "found 29 images.\n",
      "new metadata will be created / 新しいメタデータファイルが作成されます\n",
      "merge tags to metadata json.\n",
      "100%|████████████████████████████████████████| 29/29 [00:00<00:00, 37774.79it/s]\n",
      "writing metadata: /workspace/fine_tune/meta_cap_dd.json\n",
      "done!\n",
      "loading existing metadata: /workspace/fine_tune/meta_cap_dd.json\n",
      "cleaning captions and tags.\n",
      "  0%|                                                    | 0/29 [00:00<?, ?it/s]image does not have caption / メタデータにキャプションがありません: 20230224_rida_black\n",
      "image does not have caption / メタデータにキャプションがありません: 20220704_bday_doodlesdeep\n",
      "image does not have caption / メタデータにキャプションがありません: 20230215_f_the_police\n",
      "image does not have caption / メタデータにキャプションがありません: 20220803_rida\n",
      "image does not have caption / メタデータにキャプションがありません: 20230215_heart\n",
      "image does not have caption / メタデータにキャプションがありません: 20230127_tie_hair\n",
      "image does not have caption / メタデータにキャプションがありません: 20230104_rids\n",
      "image does not have caption / メタデータにキャプションがありません: 20220925_rida_bnuy\n",
      "image does not have caption / メタデータにキャプションがありません: 20230218_hu_tao_disgust\n",
      "image does not have caption / メタデータにキャプションがありません: 20230405_SaberLumiEre_flat_hb\n",
      "image does not have caption / メタデータにキャプションがありません: 20221006_rida_the_suc\n",
      "image does not have caption / メタデータにキャプションがありません: 20230222_pigtails\n",
      "image does not have caption / メタデータにキャプションがありません: 20230224_narutsuart_flat_b\n",
      "image does not have caption / メタデータにキャプションがありません: 20230131_Gooseuncharted_full_hb2\n",
      "image does not have caption / メタデータにキャプションがありません: 20230320_SaberLumiEre_flat_hb\n",
      "image does not have caption / メタデータにキャプションがありません: 20230124_mlem\n",
      "image does not have caption / メタデータにキャプションがありません: 20221030_bruh\n",
      "image does not have caption / メタデータにキャプションがありません: 20230129_fancy\n",
      "image does not have caption / メタデータにキャプションがありません: 20230320_morning\n",
      "image does not have caption / メタデータにキャプションがありません: 20230313_cyber\n",
      "image does not have caption / メタデータにキャプションがありません: 20230316_sunny\n",
      "image does not have caption / メタデータにキャプションがありません: 20230103_bunida\n",
      "image does not have caption / メタデータにキャプションがありません: 20230208_maida\n",
      "image does not have caption / メタデータにキャプションがありません: 20221231_new_year\n",
      "image does not have caption / メタデータにキャプションがありません: 20221224_xmas\n",
      "image does not have caption / メタデータにキャプションがありません: 20210827_swapped\n",
      "image does not have caption / メタデータにキャプションがありません: 20230127_cap\n",
      "image does not have caption / メタデータにキャプションがありません: 20230117_brida\n",
      "image does not have caption / メタデータにキャプションがありません: 20220731_rids\n",
      "100%|█████████████████████████████████████████| 29/29 [00:00<00:00, 2185.83it/s]\n",
      "writing metadata: /workspace/fine_tune/meta_clean.json\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "#@title ## 4.5. Create JSON file for Finetuning\n",
    "import os\n",
    "%store -r\n",
    "\n",
    "%cd {finetune_dir}\n",
    "\n",
    "#@markdown ### Define Parameter\n",
    "meta_clean = \"/workspace/fine_tune/meta_clean.json\" #@param {type:\"string\"}\n",
    "#@markdown This cell will merge all dataset label from captioning, tagging, and custom tagging into one JSON file, and later it will be used as input JSON for bucketing section.\n",
    "\n",
    "parent_folder = os.path.dirname(meta_clean)\n",
    "meta_cap_dd = f\"{parent_folder}/meta_cap_dd.json\"\n",
    "meta_cap = f\"{parent_folder}/meta_cap.json\"\n",
    "\n",
    "os.makedirs(parent_folder, exist_ok=True)\n",
    "\n",
    "if os.path.isdir(train_data_dir):\n",
    "  if any(file.endswith('.caption') for file in os.listdir(train_data_dir)):\n",
    "    !python merge_captions_to_metadata.py \\\n",
    "      {train_data_dir} \\\n",
    "      {meta_cap}\n",
    "\n",
    "  if any(file.endswith('.txt') for file in os.listdir(train_data_dir)):\n",
    "    !python merge_dd_tags_to_metadata.py \\\n",
    "      {train_data_dir} \\\n",
    "      {meta_cap_dd}\n",
    "else:\n",
    "  print(\"train_data_dir does not exist or is not a directory.\")\n",
    "\n",
    "if os.path.exists(meta_cap):\n",
    "  !python merge_dd_tags_to_metadata.py \\\n",
    "    {train_data_dir} \\\n",
    "    --in_json {meta_cap} \\\n",
    "    {meta_cap_dd}\n",
    "\n",
    "if os.path.exists(meta_cap_dd):\n",
    "  !python clean_captions_and_tags.py \\\n",
    "    {meta_cap_dd} \\\n",
    "    {meta_clean}\n",
    "elif os.path.exists(meta_cap):\n",
    "  !python clean_captions_and_tags.py \\\n",
    "    {meta_cap} \\\n",
    "    {meta_clean}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "hhgatqF3leHJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/kohya-trainer/finetune\n",
      "2023-04-07 23:03:31.409205: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-07 23:03:31.547034: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-07 23:03:32.176292: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-07 23:03:32.176373: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-07 23:03:32.176388: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "found 29 images.\n",
      "loading existing metadata: /workspace/fine_tune/meta_clean.json\n",
      "load VAE: /workspace/pre_trained_model/anyloraCheckpoint_novaeFp16Pruned.ckpt\n",
      "100%|███████████████████████████████████████████| 29/29 [00:02<00:00, 10.75it/s]\n",
      "bucket 0 (256, 832): 0\n",
      "bucket 1 (256, 896): 0\n",
      "bucket 2 (256, 960): 0\n",
      "bucket 3 (256, 1024): 0\n",
      "bucket 4 (320, 704): 0\n",
      "bucket 5 (320, 768): 0\n",
      "bucket 6 (384, 640): 0\n",
      "bucket 7 (448, 576): 19\n",
      "bucket 8 (512, 512): 9\n",
      "bucket 9 (576, 448): 1\n",
      "bucket 10 (640, 384): 0\n",
      "bucket 11 (704, 320): 0\n",
      "bucket 12 (768, 320): 0\n",
      "bucket 13 (832, 256): 0\n",
      "bucket 14 (896, 256): 0\n",
      "bucket 15 (960, 256): 0\n",
      "bucket 16 (1024, 256): 0\n",
      "mean ar error: 0.0492822853022338\n",
      "writing metadata: /workspace/fine_tune/meta_lat.json\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "#@title ## 4.6. Aspect Ratio Bucketing and Cache Latents\n",
    "%store -r\n",
    "\n",
    "# Change the working directory\n",
    "%cd {finetune_dir}\n",
    "\n",
    "#@markdown ### Define parameters\n",
    "v2 = False #@param{type:\"boolean\"}\n",
    "model_dir = \"/workspace/pre_trained_model/anyloraCheckpoint_novaeFp16Pruned.ckpt\" #@param {'type' : 'string'} \n",
    "input_json = \"/workspace/fine_tune/meta_clean.json\" #@param {'type' : 'string'} \n",
    "output_json = \"/workspace/fine_tune/meta_lat.json\"#@param {'type' : 'string'} \n",
    "batch_size = 8 #@param {'type':'integer'}\n",
    "max_data_loader_n_workers = 2 #@param {'type':'integer'}\n",
    "max_resolution = \"512,512\" #@param [\"512,512\", \"640,640\", \"768,768\"] {allow-input: false}\n",
    "mixed_precision = \"no\" #@param [\"no\", \"fp16\", \"bf16\"] {allow-input: false}\n",
    "flip_aug = False #@param{type:\"boolean\"}\n",
    "skip_existing_latents = False #@param{type:\"boolean\"}\n",
    "\n",
    "# Run script to prepare buckets and latents\n",
    "bucket_latents=f\"\"\"\n",
    "python prepare_buckets_latents.py \\\n",
    "  {train_data_dir} \\\n",
    "  {input_json} \\\n",
    "  {output_json} \\\n",
    "  {model_dir} \\\n",
    "  {\"--v2\" if v2 else \"\"} \\\n",
    "  {\"--flip_aug\" if flip_aug else \"\"} \\\n",
    "  {\"--skip_existing\" if skip_existing_latents else \"\"} \\\n",
    "  {\"--min_bucket_reso=\" + format(320) if max_resolution != \"512,512\" else \"--min_bucket_reso=\" + format(256)} \\\n",
    "  {\"--max_bucket_reso=\" + format(1280) if max_resolution != \"512,512\" else \"--max_bucket_reso=\" + format(1024)} \\\n",
    "  --batch_size {batch_size} \\\n",
    "  --max_data_loader_n_workers {max_data_loader_n_workers} \\\n",
    "  --max_resolution {max_resolution} \\\n",
    "  --mixed_precision {mixed_precision}\n",
    "  \"\"\"\n",
    "  \n",
    "f = open(\"./bucket_latents.sh\", \"w\")\n",
    "f.write(bucket_latents)\n",
    "f.close()\n",
    "!chmod +x ./bucket_latents.sh\n",
    "!./bucket_latents.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHNbl3O_NSS0"
   },
   "source": [
    "# V. Training model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "id": "ur73rlY7bEef",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_data_dir' (str)\n"
     ]
    }
   ],
   "source": [
    "#@title ## 5.1. Define Important folder\n",
    "# from google.colab import drive\n",
    "%store -r\n",
    "\n",
    "v2 = False #@param {type:\"boolean\"}\n",
    "v_parameterization = False #@param {type:\"boolean\"}\n",
    "project_name = \"mylora\" #@param {type:\"string\"}\n",
    "pretrained_model_name_or_path = \"/workspace/pre_trained_model/anyloraCheckpoint_novaeFp16Pruned.ckpt\" #@param {type:\"string\"}\n",
    "vae = \"\"  #@param {type:\"string\"}\n",
    "train_data_dir = \"/workspace/fine_tune/train_data\"  #@param {type:\"string\"}\n",
    "%store train_data_dir\n",
    "in_json = \"/workspace/fine_tune/meta_lat.json\" #@param {type:\"string\"}\n",
    "output_dir = \"/workspace/fine_tune/output\" #@param {type:\"string\"}\n",
    "resume_path = \"\"\n",
    "\n",
    "#@markdown This will ignore `output_dir` defined above, and changed to `/workspace/drive/MyDrive/fine_tune/output` by default\n",
    "# output_to_drive = False #@param {'type':'boolean'}\n",
    "\n",
    "# if output_to_drive:\n",
    "#   output_dir = \"/workspace/drive/MyDrive/fine_tune/output\"\n",
    "\n",
    "#   if not os.path.exists(\"/workspace/drive\"):\n",
    "#     drive.mount('/workspace/drive')  \n",
    "\n",
    "# Check if directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "  # Create directory if it doesn't exist\n",
    "  os.makedirs(output_dir)\n",
    "\n",
    "#V2 Inference\n",
    "inference_url = \"https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/configs/stable-diffusion/\"\n",
    "\n",
    "if v2 and not v_parameterization:\n",
    "  inference_url += \"v2-inference.yaml\"\n",
    "if v2 and v_parameterization:\n",
    "  inference_url += \"v2-inference-v.yaml\"\n",
    "\n",
    "try:\n",
    "  if v2:\n",
    "    !wget {inference_url} -O {output_dir}/{project_name}.yaml\n",
    "    print(\"File successfully downloaded\")\n",
    "except:\n",
    "  print(\"There was an error downloading the file. Please check the URL and try again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "id": "32aNPjnYAcu_",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network module: networks.lora\n",
      "networks.lora dim set to: 16\n",
      "networks.lora alpha set to: 8\n",
      "No LoRA weight loaded.\n",
      "Global learning rate:  0.0001\n",
      "Enabling LoRA for U-Net\n",
      "Enabling LoRA for Text Encoder\n",
      "UNet learning rate:  0.0001\n",
      "Text encoder learning rate:  5e-05\n",
      "Learning rate Scheduler: cosine_with_restarts\n",
      "- Number of cycles:  3\n",
      "Training comment: this_comment_will_be_stored_in_the_metadata\n"
     ]
    }
   ],
   "source": [
    "#@title ## 5.2. Define Specific LoRA Training Parameters\n",
    "%store -r\n",
    "\n",
    "#@markdown ## LoRA - Low Rank Adaptation Fine-Tuning\n",
    "\n",
    "#@markdown Some people recommend setting the `network_dim` to a higher value.\n",
    "network_dim = 16 #@param {'type':'number'}\n",
    "#@markdown For weight scaling in LoRA, it is better to set `network_alpha` the same as `network_dim` unless you know what you're doing. A lower `network_alpha` requires a higher learning rate. For example, if `network_alpha = 1`, then `unet_lr = 1e-3`.\n",
    "network_alpha = 8 #@param {'type':'number'}\n",
    "network_module = \"networks.lora\"\n",
    "\n",
    "#@markdown `network_weights` can be specified to resume training.\n",
    "network_weights = \"\" #@param {'type':'string'}\n",
    "\n",
    "#@markdown By default, both Text Encoder and U-Net LoRA modules are enabled. Use `network_train_on` to specify which module to train.\n",
    "network_train_on = \"both\" #@param ['both','unet_only', 'text_encoder_only'] {'type':'string'}\n",
    "\n",
    "#@markdown It is recommended to set the `text_encoder_lr` to a lower learning rate, such as `5e-5`, or to set `text_encoder_lr = 1/2 * unet_lr`.\n",
    "learning_rate = 1e-4 #@param {'type':'number'}\n",
    "unet_lr = 1e-4 #@param {'type':'number'}\n",
    "# text_encoder_lr = 5e-5 #@param {'type':'number'}\n",
    "text_encoder_lr = 1/2 * unet_lr\n",
    "lr_scheduler = \"cosine_with_restarts\" #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"] {allow-input: false}\n",
    "\n",
    "#@markdown If `lr_scheduler = cosine_with_restarts`, update `lr_scheduler_num_cycles`.\n",
    "lr_scheduler_num_cycles = 3 #@param {'type':'number'}\n",
    "#@markdown If `lr_scheduler = polynomial`, update `lr_scheduler_power`.\n",
    "lr_scheduler_power = 1 #@param {'type':'number'}\n",
    "\n",
    "#@markdown Check the box to not save metadata in the output model.\n",
    "no_metadata = False #@param {type:\"boolean\"}\n",
    "training_comment = \"this comment will be stored in the metadata\" #@param {'type':'string'}\n",
    "\n",
    "print(\"Loading network module:\", network_module)\n",
    "print(f\"{network_module} dim set to:\", network_dim)\n",
    "print(f\"{network_module} alpha set to:\", network_alpha)\n",
    "\n",
    "if network_weights == \"\":\n",
    "  print(\"No LoRA weight loaded.\")\n",
    "else:\n",
    "  if os.path.exists(network_weights):\n",
    "    print(\"Loading LoRA weight:\", network_weights)\n",
    "  else:\n",
    "    print(f\"{network_weights} does not exist.\")\n",
    "    network_weights =\"\"\n",
    "\n",
    "if network_train_on == \"unet_only\":\n",
    "  print(\"Enabling LoRA for U-Net.\")\n",
    "  print(\"Disabling LoRA for Text Encoder.\")\n",
    "\n",
    "print(\"Global learning rate: \", learning_rate)\n",
    "\n",
    "if network_train_on == \"unet_only\":\n",
    "  print(\"Enable LoRA for U-Net\")\n",
    "  print(\"Disable LoRA for Text Encoder\")\n",
    "  print(\"UNet learning rate: \", unet_lr) if unet_lr != 0 else \"\"\n",
    "if network_train_on == \"text_encoder_only\":\n",
    "  print(\"Disabling LoRA for U-Net\")\n",
    "  print(\"Enabling LoRA for Text Encoder\")\n",
    "  print(\"Text encoder learning rate: \", text_encoder_lr) if text_encoder_lr != 0 else \"\"\n",
    "else:\n",
    "  print(\"Enabling LoRA for U-Net\")\n",
    "  print(\"Enabling LoRA for Text Encoder\")\n",
    "  print(\"UNet learning rate: \", unet_lr) if unet_lr != 0 else \"\"\n",
    "  print(\"Text encoder learning rate: \", text_encoder_lr) if text_encoder_lr != 0 else \"\"\n",
    "\n",
    "print(\"Learning rate Scheduler:\", lr_scheduler)\n",
    "\n",
    "if lr_scheduler == \"cosine_with_restarts\":\n",
    "  print(\"- Number of cycles: \", lr_scheduler_num_cycles)\n",
    "elif lr_scheduler == \"polynomial\":\n",
    "  print(\"- Power: \", lr_scheduler_power)\n",
    "\n",
    "# Printing the training comment if metadata is not disabled and a comment is present\n",
    "if not no_metadata:\n",
    "  if training_comment: \n",
    "    training_comment = training_comment.replace(\" \", \"_\")\n",
    "    print(\"Training comment:\", training_comment)\n",
    "else:\n",
    "  print(\"Metadata won't be saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "id": "X_Rd3Eh07xlA",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/kohya-trainer\n",
      "2023-04-07 23:12:15.653101: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-07 23:12:15.798817: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-07 23:12:16.312730: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-07 23:12:16.312801: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-07 23:12:16.312811: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-04-07 23:12:18.570814: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-07 23:12:18.709553: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-07 23:12:19.279694: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-07 23:12:19.279770: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-07 23:12:19.279782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "prepare tokenizer\n",
      "update token length: 225\n",
      "Train with captions.\n",
      "loading existing metadata: /workspace/fine_tune/meta_lat.json\n",
      "metadata has bucket info, enable bucketing / メタデータにbucket情報があるためbucketを有効にします\n",
      "using bucket info in metadata / メタデータ内のbucket情報を使います\n",
      "loading image sizes.\n",
      "100%|███████████████████████████████████████| 29/29 [00:00<00:00, 711314.71it/s]\n",
      "make buckets\n",
      "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
      "bucket 0: resolution (448, 576), count: 190\n",
      "bucket 1: resolution (512, 512), count: 90\n",
      "bucket 2: resolution (576, 448), count: 10\n",
      "mean ar error (without repeats): 0.0\n",
      "prepare accelerator\n",
      "Using accelerator 0.15.0 or above.\n",
      "load StableDiffusion checkpoint\n",
      "loading u-net: <All keys matched successfully>\n",
      "loading vae: <All keys matched successfully>\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'logit_scale', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm1.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "loading text encoder: <All keys matched successfully>\n",
      "Replace CrossAttention.forward to use xformers\n",
      "import network module: networks.lora\n",
      "create LoRA for Text Encoder: 72 modules.\n",
      "create LoRA for U-Net: 192 modules.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "prepare optimizer, data loader etc.\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
      "================================================================================\n",
      "/workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64'), PosixPath('/workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages/cv2/../../lib64')}\n",
      "  warn(\n",
      "/workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/paths.py:105: UserWarning: /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(\n",
      "/workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary /workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda116.so...\n",
      "use 8-bit Adam optimizer\n",
      "override steps. steps for 20 epochs is / 指定エポックまでのステップ数: 1160\n",
      "running training / 学習開始\n",
      "  num train images * repeats / 学習画像の数×繰り返し回数: 290\n",
      "  num reg images / 正則化画像の数: 0\n",
      "  num batches per epoch / 1epochのバッチ数: 58\n",
      "  num epochs / epoch数: 20\n",
      "  batch size per device / バッチサイズ: 5\n",
      "  total train batch size (with parallel & distributed & accumulation) / 総バッチサイズ（並列学習、勾配合計含む）: 5\n",
      "  gradient accumulation steps / 勾配を合計するステップ数 = 1\n",
      "  total optimization steps / 学習ステップ数: 1160\n",
      "steps:   0%|                                           | 0/1160 [00:00<?, ?it/s]epoch 1/20\n",
      "/workspace/stable-diffusion-webui/venv/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "steps:   5%|█                    | 58/1160 [00:16<05:06,  3.60it/s, loss=0.0666]epoch 2/20\n",
      "steps:  10%|██                  | 116/1160 [00:31<04:40,  3.72it/s, loss=0.0631]epoch 3/20\n",
      "steps:  15%|███                 | 174/1160 [00:46<04:21,  3.77it/s, loss=0.0599]epoch 4/20\n",
      "steps:  20%|████                | 232/1160 [01:00<04:03,  3.81it/s, loss=0.0525]epoch 5/20\n",
      "steps:  25%|█████               | 290/1160 [01:15<03:47,  3.82it/s, loss=0.0661]epoch 6/20\n",
      "steps:  30%|██████              | 348/1160 [01:30<03:31,  3.83it/s, loss=0.0666]saving checkpoint: /workspace/fine_tune/output/mylora-000006.safetensors\n",
      "epoch 7/20\n",
      "steps:  35%|███████             | 406/1160 [01:45<03:16,  3.83it/s, loss=0.0602]epoch 8/20\n",
      "steps:  40%|████████▍            | 464/1160 [02:00<03:01,  3.83it/s, loss=0.066]epoch 9/20\n",
      "steps:  45%|█████████           | 522/1160 [02:15<02:45,  3.85it/s, loss=0.0599]epoch 10/20\n",
      "steps:  50%|██████████          | 580/1160 [02:30<02:30,  3.85it/s, loss=0.0623]epoch 11/20\n",
      "steps:  55%|███████████         | 638/1160 [02:45<02:15,  3.85it/s, loss=0.0617]epoch 12/20\n",
      "steps:  60%|████████████        | 696/1160 [03:00<02:00,  3.85it/s, loss=0.0648]saving checkpoint: /workspace/fine_tune/output/mylora-000012.safetensors\n",
      "epoch 13/20\n",
      "steps:  65%|██████████████▎       | 754/1160 [03:15<01:45,  3.85it/s, loss=0.06]epoch 14/20\n",
      "steps:  70%|██████████████      | 812/1160 [03:30<01:30,  3.85it/s, loss=0.0643]epoch 15/20\n",
      "steps:  75%|███████████████     | 870/1160 [03:45<01:15,  3.85it/s, loss=0.0558]epoch 16/20\n",
      "steps:  80%|████████████████    | 928/1160 [04:01<01:00,  3.85it/s, loss=0.0594]epoch 17/20\n",
      "steps:  85%|█████████████████   | 986/1160 [04:16<00:45,  3.85it/s, loss=0.0597]epoch 18/20\n",
      "steps:  90%|█████████████████  | 1044/1160 [04:31<00:30,  3.85it/s, loss=0.0561]saving checkpoint: /workspace/fine_tune/output/mylora-000018.safetensors\n",
      "epoch 19/20\n",
      "steps:  95%|██████████████████ | 1102/1160 [04:46<00:15,  3.85it/s, loss=0.0546]epoch 20/20\n",
      "steps: 100%|███████████████████| 1160/1160 [05:01<00:00,  3.85it/s, loss=0.0565]save trained model to /workspace/fine_tune/output/mylora.safetensors\n",
      "model saved.\n",
      "steps: 100%|███████████████████| 1160/1160 [05:01<00:00,  3.85it/s, loss=0.0565]\n"
     ]
    }
   ],
   "source": [
    "# from prettytable import PrettyTable\n",
    "import textwrap\n",
    "import yaml\n",
    "\n",
    "%store -r\n",
    "\n",
    "#@title ## 5.2. Start Fine-Tuning\n",
    "#@markdown ### Define Parameter\n",
    "train_batch_size = 5 #@param {type:\"number\"}\n",
    "num_epochs = 20 #@param {type:\"number\"}\n",
    "dataset_repeats = 10 #@param {type:\"number\"}\n",
    "mixed_precision = \"fp16\" #@param [\"no\",\"fp16\",\"bf16\"] {allow-input: false}\n",
    "save_precision = \"fp16\" #@param [\"float\", \"fp16\", \"bf16\"] {allow-input: false}\n",
    "save_n_epochs_type = \"save_n_epoch_ratio\" #@param [\"save_every_n_epochs\", \"save_n_epoch_ratio\"] {allow-input: false}\n",
    "save_n_epochs_type_value = 3 #@param {type:\"number\"}\n",
    "save_model_as = \"safetensors\" #@param [\"ckpt\", \"pt\", \"safetensors\"] {allow-input: false}\n",
    "resolution = 512 #@param {type:\"slider\", min:512, max:1024, step:128}\n",
    "max_token_length = 225 #@param {type:\"number\"}\n",
    "clip_skip = 2 #@param {type:\"number\"}\n",
    "use_8bit_adam = True #@param {type:\"boolean\"}\n",
    "gradient_checkpointing = False #@param {type:\"boolean\"}\n",
    "gradient_accumulation_steps = 1 #@param {type:\"number\"}\n",
    "seed = 0 #@param {type:\"number\"}\n",
    "logging_dir = \"/workspace/fine_tune/logs\"\n",
    "log_prefix = project_name\n",
    "additional_argument = \"--shuffle_caption --xformers\" #@param {type:\"string\"}\n",
    "print_hyperparameter = False #@param {type:\"boolean\"}\n",
    "\n",
    "%cd {repo_dir}\n",
    "\n",
    "train_command=f\"\"\"\n",
    "accelerate launch --config_file={accelerate_config} --num_cpu_threads_per_process=8 train_network.py \\\n",
    "  {\"--v2\" if v2 else \"\"} \\\n",
    "  {\"--v_parameterization\" if v2 and v_parameterization else \"\"} \\\n",
    "  --network_dim={network_dim} \\\n",
    "  --network_alpha={network_alpha} \\\n",
    "  --network_module={network_module} \\\n",
    "  {\"--network_weights=\" + network_weights if network_weights else \"\"} \\\n",
    "  {\"--network_train_unet_only\" if network_train_on == \"unet_only\" else \"\"} \\\n",
    "  {\"--network_train_text_encoder_only\" if network_train_on == \"text_encoder_only\" else \"\"} \\\n",
    "  --learning_rate={learning_rate} \\\n",
    "  {\"--unet_lr=\" + format(unet_lr) if unet_lr else \"\"} \\\n",
    "  {\"--text_encoder_lr=\" + format(text_encoder_lr) if text_encoder_lr else \"\"} \\\n",
    "  {\"--no_metadata\" if no_metadata else \"\"} \\\n",
    "  {\"--training_comment=\" + training_comment if training_comment and not no_metadata else \"\"} \\\n",
    "  --lr_scheduler={lr_scheduler} \\\n",
    "  {\"--lr_scheduler_num_cycles=\" + format(lr_scheduler_num_cycles) if lr_scheduler == \"cosine_with_restarts\" else \"\"} \\\n",
    "  {\"--lr_scheduler_power=\" + format(lr_scheduler_power) if lr_scheduler == \"polynomial\" else \"\"} \\\n",
    "  --pretrained_model_name_or_path={pretrained_model_name_or_path} \\\n",
    "  {\"--vae=\" + vae if vae else \"\"} \\\n",
    "  --train_data_dir={train_data_dir} \\\n",
    "  --in_json={in_json} \\\n",
    "  --output_dir={output_dir} \\\n",
    "  {\"--keep_tokens=\" + format(keep_tokens) if \"keep_tokens\" in locals() or \"keep_tokens\" in globals() else \"\"} \\\n",
    "  {\"--resume=\" + resume_path if resume_path else \"\"} \\\n",
    "  {\"--output_name=\" + project_name if project_name else \"\"} \\\n",
    "  --mixed_precision={mixed_precision} \\\n",
    "  --save_precision={save_precision} \\\n",
    "  {\"--save_every_n_epochs=\" + format(save_n_epochs_type_value) if save_n_epochs_type==\"save_every_n_epochs\" else \"\"} \\\n",
    "  {\"--save_n_epoch_ratio=\" + format(save_n_epochs_type_value) if save_n_epochs_type==\"save_n_epoch_ratio\" else \"\"} \\\n",
    "  --save_model_as={save_model_as} \\\n",
    "  --resolution={resolution} \\\n",
    "  --train_batch_size={train_batch_size} \\\n",
    "  --max_token_length={max_token_length} \\\n",
    "  {\"--use_8bit_adam\" if use_8bit_adam else \"\"} \\\n",
    "  --dataset_repeats={dataset_repeats} \\\n",
    "  --max_train_epochs={num_epochs} \\\n",
    "  {\"--seed=\" + format(seed) if seed > 0 else \"\"} \\\n",
    "  {\"--gradient_checkpointing\" if gradient_checkpointing else \"\"} \\\n",
    "  {\"--gradient_accumulation_steps=\" + format(gradient_accumulation_steps) } \\\n",
    "  {\"--clip_skip=\" + format(clip_skip) if v2 == False else \"\"} \\\n",
    "  --logging_dir={logging_dir} \\\n",
    "  --log_prefix={log_prefix} \\\n",
    "  {additional_argument}\n",
    "  \"\"\"\n",
    "\n",
    "debug_params = [\"v2\", \\\n",
    "                \"v_parameterization\", \\\n",
    "                \"network_dim\", \\\n",
    "                \"network_alpha\", \\\n",
    "                \"network_module\", \\\n",
    "                \"network_weights\", \\\n",
    "                \"network_train_on\", \\\n",
    "                \"learning_rate\", \\\n",
    "                \"unet_lr\", \\\n",
    "                \"text_encoder_lr\", \\\n",
    "                \"no_metadata\", \\\n",
    "                \"training_comment\", \\\n",
    "                \"lr_scheduler\", \\\n",
    "                \"lr_scheduler_num_cycles\", \\\n",
    "                \"lr_scheduler_power\", \\\n",
    "                \"pretrained_model_name_or_path\", \\\n",
    "                \"vae\", \\\n",
    "                \"train_data_dir\", \\\n",
    "                \"in_json\", \\\n",
    "                \"output_dir\", \\\n",
    "                \"keep_tokens\" if \"keep_tokens\" in locals() or \"keep_tokens\" in globals() else \"\", \\\n",
    "                \"resume_path\", \\\n",
    "                \"project_name\", \\\n",
    "                \"mixed_precision\", \\\n",
    "                \"save_precision\", \\\n",
    "                \"save_n_epochs_type\", \\\n",
    "                \"save_n_epochs_type_value\", \\\n",
    "                \"save_model_as\", \\\n",
    "                \"resolution\", \\\n",
    "                \"train_batch_size\", \\\n",
    "                \"max_token_length\", \\\n",
    "                \"use_8bit_adam\", \\\n",
    "                \"dataset_repeats\", \\\n",
    "                \"num_epochs\", \\\n",
    "                \"seed\", \\\n",
    "                \"gradient_checkpointing\", \\\n",
    "                \"gradient_accumulation_steps\", \\\n",
    "                \"clip_skip\", \\\n",
    "                \"logging_dir\", \\\n",
    "                \"log_prefix\", \\\n",
    "                \"additional_argument\"]\n",
    "\n",
    "# if print_hyperparameter:\n",
    "#     table = PrettyTable()\n",
    "#     table.field_names = [\"Hyperparameter\", \"Value\"]\n",
    "#     for params in debug_params:\n",
    "#         if params != \"\":\n",
    "#             if globals()[params] == \"\":\n",
    "#                 value = \"False\"\n",
    "#             else:\n",
    "#                 value = globals()[params]\n",
    "#             table.add_row([params, value])\n",
    "#     table.align = \"l\"\n",
    "#     print(table)\n",
    "\n",
    "#     arg_list = train_command.split()\n",
    "#     mod_train_command = {'command': arg_list}\n",
    "    \n",
    "#     train_folder = os.path.dirname(output_dir)\n",
    "    \n",
    "#     # save the YAML string to a file\n",
    "#     with open(str(train_folder)+'/finetune_lora_cmd.yaml', 'w') as f:\n",
    "#         yaml.dump(mod_train_command, f)\n",
    "\n",
    "f = open(\"./train.sh\", \"w\")\n",
    "f.write(train_command)\n",
    "f.close()\n",
    "!chmod +x ./train.sh\n",
    "!./train.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /workspace/pre_trained_model/anyloraCheckpoint_novaeFp16Pruned.ckpt /workspace/stable-diffusion-webui/models/Stable-diffusion/anylora.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv /workspace/fine_tune/output/mylora.safetensors /workspace/stable-diffusion-webui/models/Lora/mylora.safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reMcN0bM_o53"
   },
   "source": [
    "# VI. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "eRpHO0t-3xoY"
   },
   "outputs": [],
   "source": [
    "#@title ## 6.1. Validating LoRA Weights\n",
    "\n",
    "#@markdown Now you can check if your LoRA trained properly. \n",
    "\n",
    "#@markdown  If you used `clip_skip = 2` during training, the values of `lora_te_text_model_encoder_layers_11_*` will be `0.0`, this is normal. These layers are not trained at this value of `Clip Skip`.\n",
    "network_weights = \"/workspace/fine_tune/output/neurosama.safetensors\" #@param {'type':'string'}\n",
    "no_verbose = True #@param {type:\"boolean\"}\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from safetensors.torch import load_file\n",
    "from safetensors.torch import safe_open\n",
    "import library.model_util as model_util\n",
    "\n",
    "print(\"Loading LoRA weight:\", network_weights)\n",
    "\n",
    "def main(file, verbose:bool):\n",
    "  if not verbose:\n",
    "    sd = load_file(file) if os.path.splitext(file)[1] == '.safetensors' else torch.load(file, map_location='cuda')\n",
    "    values = []\n",
    "\n",
    "    keys = list(sd.keys())\n",
    "    for key in keys:\n",
    "      if 'lora_up' in key or 'lora_down' in key:\n",
    "        values.append((key, sd[key]))\n",
    "    print(f\"number of LoRA modules: {len(values)}\")\n",
    "\n",
    "    for key, value in values:\n",
    "      value = value.to(torch.float32)\n",
    "      print(f\"{key},{torch.mean(torch.abs(value))},{torch.min(torch.abs(value))}\")\n",
    "  \n",
    "  if model_util.is_safetensors(file):\n",
    "      with safe_open(file, framework=\"pt\") as f:\n",
    "          metadata = f.metadata()\n",
    "      if metadata is not None:\n",
    "        if not verbose:\n",
    "          metadata[\"ss_bucket_info\"] = json.loads(metadata[\"ss_bucket_info\"].replace(\"\\\\\", \"\"))\n",
    "          metadata[\"ss_tag_frequency\"] = json.loads(metadata[\"ss_tag_frequency\"].replace(\"\\\\\", \"\"))\n",
    "        metadata[\"ss_dataset_dirs\"] = json.loads(metadata[\"ss_dataset_dirs\"].replace(\"\\\\\", \"\"))\n",
    "        metadata[\"ss_reg_dataset_dirs\"] = json.loads(metadata[\"ss_reg_dataset_dirs\"].replace(\"\\\\\", \"\"))\n",
    "        print(f\"\\nLoad metadata for: {file}\")\n",
    "        print(json.dumps(metadata, indent=4))\n",
    "  else:\n",
    "      print(\"No metadata saved, your model is not in safetensors format\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main(network_weights, no_verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "x6t8PbnF0gbg"
   },
   "outputs": [],
   "source": [
    "#@title ## 6.2. Inference\n",
    "%store -r\n",
    "\n",
    "#@markdown LoRA Config\n",
    "network_weights = \"/workspace/fine_tune/output/neurosama.safetensors\" #@param {'type':'string'}\n",
    "network_module = \"networks.lora\"\n",
    "network_mul = 0.6 #@param {'type':'number'}\n",
    "\n",
    "#@markdown Other Config\n",
    "v2 = False #@param {type:\"boolean\"}\n",
    "v_parameterization = False #@param {type:\"boolean\"}\n",
    "instance_prompt = \"\" #@param {type: \"string\"}\n",
    "prompt = \"masterpiece, best quality, 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\" #@param {type: \"string\"}\n",
    "negative = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\" #@param {type: \"string\"}\n",
    "model = \"/workspace/pre_trained_model/Anything-v3-1.safetensors\" #@param {type: \"string\"}\n",
    "vae = \"\" #@param {type: \"string\"}\n",
    "outdir = \"/workspace/tmp\" #@param {type: \"string\"}\n",
    "scale = 6 #@param {type: \"slider\", min: 1, max: 40}\n",
    "sampler = \"ddim\" #@param [\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
    "steps = 28 #@param {type: \"slider\", min: 1, max: 100}\n",
    "precision = \"fp16\" #@param [\"fp16\", \"bf16\"] {allow-input: false}\n",
    "width = 512 #@param {type: \"integer\"}\n",
    "height = 768 #@param {type: \"integer\"}\n",
    "images_per_prompt = 4 #@param {type: \"integer\"}\n",
    "batch_size = 4 #@param {type: \"integer\"}\n",
    "clip_skip = 2 #@param {type: \"slider\", min: 1, max: 40}\n",
    "seed = -1 #@param {type: \"integer\"}\n",
    "\n",
    "final_prompt = f\"{instance_prompt}, {prompt} --n {negative}\" if instance_prompt else f\"{prompt} --n {negative}\"\n",
    "\n",
    "%cd {repo_dir}\n",
    "\n",
    "!python gen_img_diffusers.py \\\n",
    "  {\"--v2\" if v2 else \"\"} \\\n",
    "  {\"--v_parameterization\" if v2 and v_parameterization else \"\"} \\\n",
    "  --network_module={network_module} \\\n",
    "  --network_weight={network_weights} \\\n",
    "  --network_mul={network_mul} \\\n",
    "  --ckpt={model} \\\n",
    "  --outdir={outdir} \\\n",
    "  --xformers \\\n",
    "  {\"--vae=\" + vae if vae else \"\"} \\\n",
    "  --{precision} \\\n",
    "  --W={width} \\\n",
    "  --H={height} \\\n",
    "  {\"--seed=\" + format(seed) if seed > 0 else \"\"} \\\n",
    "  --scale={scale} \\\n",
    "  --sampler={sampler} \\\n",
    "  --steps={steps} \\\n",
    "  --max_embeddings_multiples=3 \\\n",
    "  --batch_size={batch_size} \\\n",
    "  --images_per_prompt={images_per_prompt} \\\n",
    "  {\"--clip_skip=\" + format(clip_skip) if v2 == False else \"\"} \\\n",
    "  --prompt=\"{final_prompt}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Rt7CKCog_4tm"
   },
   "outputs": [],
   "source": [
    "#@title ## 6.3. Visualize loss graph (Optional)\n",
    "training_logs_path = \"/workspace/fine_tune/logs\" #@param {type : \"string\"}\n",
    "\n",
    "%cd /workspace/kohya-trainer\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {training_logs_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6ckzE2GWudi"
   },
   "source": [
    "# VII. Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "rLdEpPKTbI1I"
   },
   "outputs": [],
   "source": [
    "#@title ## 7.1. Compressing model or dataset\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "zip_module = \"zipfile\" #@param [\"zipfile\", \"shutil\", \"pyminizip\", \"zip\"]\n",
    "directory_to_zip = '/workspace/fine_tune/output' #@param {type: \"string\"}\n",
    "output_filename = '/workspace/output.zip' #@param {type: \"string\"}\n",
    "password = \"\" #@param {type: \"string\"}\n",
    "\n",
    "if zip_module == \"zipfile\":\n",
    "    with zipfile.ZipFile(output_filename, 'w') as zip:\n",
    "        for directory_to_zip, dirs, files in os.walk(directory_to_zip):\n",
    "            for file in files:\n",
    "                zip.write(os.path.join(directory_to_zip, file))\n",
    "elif zip_module == \"shutil\":\n",
    "    shutil.make_archive(output_filename, 'zip', directory_to_zip)\n",
    "elif zip_module == \"pyminizip\":\n",
    "    !pip install pyminizip\n",
    "    import pyminizip\n",
    "    for root, dirs, files in os.walk(directory_to_zip):\n",
    "        for file in files:\n",
    "            pyminizip.compress(os.path.join(root, file), \"\", os.path.join(\"*\",output_filename), password, 5)\n",
    "elif zip_module == \"zip\":\n",
    "    !zip -rv -q -j {output_filename} {directory_to_zip}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyIl9BhNXKUq"
   },
   "source": [
    "# VIII. Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "QTXsM170GUpk"
   },
   "outputs": [],
   "source": [
    "#@title ## 8.1. Define your Huggingface Repo\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
    "%store -r\n",
    "\n",
    "api = HfApi()\n",
    "user = api.whoami(write_token)\n",
    "\n",
    "#@markdown #### Fill this if you want to upload to your organization, or just leave it empty.\n",
    "\n",
    "orgs_name = \"\" #@param{type:\"string\"}\n",
    "\n",
    "#@markdown #### If your model/dataset repo didn't exist, it will automatically create your repo.\n",
    "model_name = \"your-model-name\" #@param{type:\"string\"}\n",
    "dataset_name = \"your-dataset-name\" #@param{type:\"string\"}\n",
    "make_this_model_private = True #@param{type:\"boolean\"}\n",
    "\n",
    "if orgs_name == \"\":\n",
    "  model_repo = user['name']+\"/\"+model_name.strip()\n",
    "  datasets_repo = user['name']+\"/\"+dataset_name.strip()\n",
    "else:\n",
    "  model_repo = orgs_name+\"/\"+model_name.strip()\n",
    "  datasets_repo = orgs_name+\"/\"+dataset_name.strip()\n",
    "\n",
    "if model_name != \"\":\n",
    "  try:\n",
    "      validate_repo_id(model_repo)\n",
    "      api.create_repo(repo_id=model_repo, \n",
    "                      private=make_this_model_private)\n",
    "      print(\"Model Repo didn't exists, creating repo\")\n",
    "      print(\"Model Repo: \",model_repo,\"created!\\n\")\n",
    "\n",
    "  except HfHubHTTPError as e:\n",
    "      print(f\"Model Repo: {model_repo} exists, skipping create repo\\n\")\n",
    "\n",
    "if dataset_name != \"\":\n",
    "  try:\n",
    "      validate_repo_id(datasets_repo)\n",
    "      api.create_repo(repo_id=datasets_repo,\n",
    "                      repo_type=\"dataset\",\n",
    "                      private=make_this_model_private)\n",
    "      print(\"Dataset Repo didn't exists, creating repo\")\n",
    "      print(\"Dataset Repo\",datasets_repo,\"created!\\n\")\n",
    "\n",
    "  except HfHubHTTPError as e:\n",
    "      print(f\"Dataset repo: {datasets_repo} exists, skipping create repo\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUNkWbMHcbiL"
   },
   "source": [
    "## 8.2. Upload with `hf_hub`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "CIeoJA-eO-8t"
   },
   "outputs": [],
   "source": [
    "#@title ### 8.2.1. Upload LoRA\n",
    "from huggingface_hub import HfApi\n",
    "from pathlib import Path\n",
    "%store -r\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "#@markdown #### This will be uploaded to model repo\n",
    "model_path = \"/workspace/fine_tune/output\" #@param {type :\"string\"}\n",
    "path_in_repo = \"neurosama_v3\" #@param {type :\"string\"}\n",
    "\n",
    "#@markdown #### Other Information\n",
    "commit_message = \"\" #@param {type :\"string\"}\n",
    "\n",
    "if not commit_message:\n",
    "  commit_message = \"feat: upload \"+project_name+\" lora model\"\n",
    "def upload_model(model_paths, is_folder :bool):\n",
    "  path_obj = Path(model_paths)\n",
    "  trained_model = path_obj.parts[-1]\n",
    "  \n",
    "  if path_in_repo:\n",
    "    trained_model = path_in_repo\n",
    "    \n",
    "  if is_folder == True:\n",
    "    print(f\"Uploading {trained_model} to https://huggingface.co/\"+model_repo)\n",
    "    print(f\"Please wait...\")\n",
    "    \n",
    "    api.upload_folder(\n",
    "        folder_path=model_paths,\n",
    "        path_in_repo=trained_model,\n",
    "        repo_id=model_repo,\n",
    "        commit_message=commit_message,\n",
    "        ignore_patterns=\".ipynb_checkpoints\"\n",
    "        )\n",
    "    print(f\"Upload success, located at https://huggingface.co/\"+model_repo+\"/tree/main\\n\")\n",
    "  else: \n",
    "    print(f\"Uploading {trained_model} to https://huggingface.co/\"+model_repo)\n",
    "    print(f\"Please wait...\")\n",
    "            \n",
    "    api.upload_file(\n",
    "        path_or_fileobj=model_paths,\n",
    "        path_in_repo=trained_model,\n",
    "        repo_id=model_repo,\n",
    "        commit_message=commit_message,\n",
    "        )\n",
    "        \n",
    "    print(f\"Upload success, located at https://huggingface.co/\"+model_repo+\"/blob/main/\"+trained_model+\"\\n\")\n",
    "      \n",
    "def upload():\n",
    "    if model_path.endswith((\".ckpt\", \".safetensors\", \".pt\")):\n",
    "      upload_model(model_path, False)\n",
    "    else:\n",
    "      upload_model(model_path, True)\n",
    "\n",
    "upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "IW-hS9jnmf-E"
   },
   "outputs": [],
   "source": [
    "#@title ### 8.2.2. Upload Dataset\n",
    "from huggingface_hub import HfApi\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "#@markdown #### This will be compressed to zip and  uploaded to datasets repo, leave it empty if not necessary\n",
    "train_data_path = \"/workspace/fine_tune/train_data\" #@param {type :\"string\"}\n",
    "meta_lat_path = \"/workspace/fine_tune/meta_lat.json\" #@param {type :\"string\"}\n",
    "#@markdown ##### `Nerd stuff, only if you want to save training logs`\n",
    "logs_path = \"/workspace/fine_tune/logs\" #@param {type :\"string\"}\n",
    "#@markdown #### Delete zip after upload\n",
    "delete_zip = True #@param {type :\"boolean\"}\n",
    "\n",
    "if project_name !=\"\":\n",
    "  tmp_dataset = \"/workspace/fine_tune/\"+project_name+\"_dataset\"\n",
    "else:\n",
    "  tmp_dataset = \"/workspace/fine_tune/tmp_dataset\"\n",
    "\n",
    "tmp_train_data = tmp_dataset + \"/train_data\"\n",
    "dataset_zip = tmp_dataset + \".zip\"\n",
    "\n",
    "#@markdown #### Other Information\n",
    "commit_message = \"\" #@param {type :\"string\"}\n",
    "\n",
    "if not commit_message:\n",
    "  commit_message = \"feat: upload \"+project_name+\" dataset and logs\"\n",
    "\n",
    "os.makedirs(tmp_dataset, exist_ok=True)\n",
    "os.makedirs(tmp_train_data, exist_ok=True)\n",
    "\n",
    "def upload_dataset(dataset_paths, is_zip : bool):\n",
    "  path_obj = Path(dataset_paths)\n",
    "  dataset_name = path_obj.parts[-1]\n",
    "\n",
    "  if is_zip:\n",
    "    print(f\"Uploading {dataset_name} to https://huggingface.co/datasets/\"+datasets_repo)\n",
    "    print(f\"Please wait...\")\n",
    "\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=dataset_paths,\n",
    "        path_in_repo=dataset_name,\n",
    "        repo_id=datasets_repo,\n",
    "        repo_type=\"dataset\",\n",
    "        commit_message=commit_message,\n",
    "    )\n",
    "    print(f\"Upload success, located at https://huggingface.co/datasets/\"+datasets_repo+\"/blob/main/\"+dataset_name+\"\\n\")\n",
    "  else:\n",
    "    print(f\"Uploading {dataset_name} to https://huggingface.co/datasets/\"+datasets_repo)\n",
    "    print(f\"Please wait...\")\n",
    "\n",
    "    api.upload_folder(\n",
    "        folder_path=dataset_paths,\n",
    "        path_in_repo=dataset_name,\n",
    "        repo_id=datasets_repo,\n",
    "        repo_type=\"dataset\",\n",
    "        commit_message=commit_message,\n",
    "        ignore_patterns=\".ipynb_checkpoints\",\n",
    "    )\n",
    "    print(f\"Upload success, located at https://huggingface.co/datasets/\"+datasets_repo+\"/tree/main/\"+dataset_name+\"\\n\")\n",
    "  \n",
    "def zip_file(tmp):\n",
    "    zipfiles = tmp + \".zip\" \n",
    "    with zipfile.ZipFile(zipfiles, 'w') as zip:\n",
    "      for tmp, dirs, files in os.walk(tmp):\n",
    "          for file in files:\n",
    "              zip.write(os.path.join(tmp, file))\n",
    "\n",
    "def move(src_path, dst_path, is_metadata: bool):\n",
    "  files_to_move = [\"meta_cap.json\", \\\n",
    "                   \"meta_cap_dd.json\", \\\n",
    "                   \"meta_lat.json\", \\\n",
    "                   \"meta_clean.json\", \\\n",
    "                   \"meta_final.json\"]\n",
    "\n",
    "  if os.path.exists(src_path):\n",
    "    shutil.move(src_path, dst_path)\n",
    "\n",
    "  if is_metadata:\n",
    "    parent_meta_path = os.path.dirname(src_path)\n",
    "\n",
    "    for filename in os.listdir(parent_meta_path):\n",
    "      file_path = os.path.join(parent_meta_path, filename)\n",
    "      if filename in files_to_move:\n",
    "        shutil.move(file_path, dst_path)\n",
    "\n",
    "def upload():\n",
    "  if train_data_path !=\"\" and meta_lat_path !=\"\":\n",
    "    move(train_data_path, tmp_train_data, False)\n",
    "    move(meta_lat_path, tmp_dataset, True)\n",
    "    zip_file(tmp_dataset)\n",
    "    upload_dataset(dataset_zip, True)\n",
    "    if delete_zip:\n",
    "      os.remove(dataset_zip)\n",
    "  if logs_path !=\"\":\n",
    "    upload_dataset(logs_path, False)\n",
    "\n",
    "upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKZpg4keWS5c"
   },
   "source": [
    "## 8.3. Commit using Git (Alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "6nBlrOrytO9F"
   },
   "outputs": [],
   "source": [
    "#@title ### 8.3.1. Clone Repository\n",
    "\n",
    "clone_model = True #@param {'type': 'boolean'}\n",
    "clone_dataset = True #@param {'type': 'boolean'}\n",
    "\n",
    "!git lfs install --skip-smudge\n",
    "!export GIT_LFS_SKIP_SMUDGE=1\n",
    "\n",
    "if clone_model:\n",
    "  !git clone https://huggingface.co/{model_repo} /workspace/{model_name}\n",
    "  \n",
    "if clone_dataset:\n",
    "  !git clone https://huggingface.co/datasets/{datasets_repo} /workspace/{dataset_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7bJev4PzOFFB"
   },
   "outputs": [],
   "source": [
    "#@title ### 8.3.2. Commit using Git \n",
    "%cd {root_dir}\n",
    "\n",
    "#@markdown Tick which repo you want to commit\n",
    "commit_model = True #@param {'type': 'boolean'}\n",
    "commit_dataset = True #@param {'type': 'boolean'}\n",
    "\n",
    "#@markdown Set **git commit identity**\n",
    "email = \"your-email-here\" #@param {'type': 'string'}\n",
    "name = \"your-username-here\" #@param {'type': 'string'}\n",
    "#@markdown Set **commit message**\n",
    "commit_message = \"\" #@param {type :\"string\"}\n",
    "\n",
    "if not commit_message:\n",
    "  commit_message = \"feat: upload \"+project_name+\" lora model and dataset\"\n",
    "\n",
    "!git config --global user.email \"{email}\"\n",
    "!git config --global user.name \"{name}\"\n",
    "\n",
    "def commit(repo_folder, commit_message):\n",
    "  %cd {root_dir}/{repo_folder}\n",
    "  !git lfs install\n",
    "  !huggingface-cli lfs-enable-largefiles .\n",
    "  !git add .\n",
    "  !git commit -m \"{commit_message}\"\n",
    "  !git push\n",
    "\n",
    "commit(model_name, commit_message)\n",
    "commit(dataset_name, commit_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFYq1HZqD7sy"
   },
   "source": [
    "# Become a supporter!\n",
    "[![ko-fi](https://ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/linaqruf)\n",
    "<a href=\"https://saweria.co/linaqruf\"><img alt=\"Saweria\" src=\"https://img.shields.io/badge/Saweria-7B3F00?style=for-the-badge&logo=ko-fi&logoColor=white\"/></a>\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
